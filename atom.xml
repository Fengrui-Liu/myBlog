<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Fengrui Liu&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.liufr.com/"/>
  <updated>2019-09-16T08:29:00.668Z</updated>
  <id>http://www.liufr.com/</id>
  
  <author>
    <name>Fengrui Liu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://www.liufr.com/2019/09/16/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/2019-09-16-%E5%9F%BA%E4%BA%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B%E7%9A%84RNN%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B/"/>
    <id>http://www.liufr.com/2019/09/16/数据挖掘/2019-09-16-基于注意力模型的RNN时序预测/</id>
    <published>2019-09-16T08:29:00.668Z</published>
    <updated>2019-09-16T08:29:00.668Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>时间注意力模型下的时序预测.md</title>
    <link href="http://www.liufr.com/2019/09/12/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/2019-09-12-%E6%97%B6%E9%97%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B%E4%B8%8B%E7%9A%84%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B/"/>
    <id>http://www.liufr.com/2019/09/12/数据挖掘/2019-09-12-时间注意力模型下的时序预测/</id>
    <published>2019-09-11T16:00:00.000Z</published>
    <updated>2019-09-16T08:29:07.271Z</updated>
    
    <content type="html"><![CDATA[<p>Multi-Horizon Time Series Forecasting with Temporal Attention Learning(KDD’ 19)</p><p>文章阐述了历史信息中的时间模式对长时间序列的预测是至关重要的。传统方法中手动设置时间依赖从而探索相关的时序模式是很不靠谱的，本文用DNN的方法提出了端到端的多视界模型来进行预测，从时间的注意力上来更好地捕获历史数据的潜在模式，还提出了一种多模型的融合机制可以结合不同历史数据的特征来更好地预测。</p><p>multi-horizon代表的意思就是在未来时间内进行多步预测。这可以用来进行资源规划的预测以及决策。这对LSTM提出了挑战，因为当前的输入源于历史信息和未来动态输入的变量，我们的可以精确预测是因为，可以通过对潜在模型的适当表示以及历史数据时间模式的参照。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="基础编码-解码结构"><a href="#基础编码-解码结构" class="headerlink" title="基础编码-解码结构"></a>基础编码-解码结构</h3><p>采用的是seq2seq流水线结构来编码历史/未来的输入变量，然后解码用于未来的预测</p><p><img src="pictures/Multi-Horizon&#32;LSTM.jpg" alt></p><p>如上图所示，编码过程是一个两层的LSTM架构，可以将历史信息匹配到隐藏层的上一层的$h_{t-1}$。其中$x_t$代表每一个时间戳中的输入，隐藏层就是$h_t$，内部的门(gate cell)就是$i_t,f_t,o_t,c_t$<br>$$<br>\begin{array}{l}{i_{t}=\sigma\left(W_{i x} x_{t}+W_{i m} m_{t-1}\right)} \ {f_{t}=\sigma\left(W_{f x} x_{t}+W_{f m} m_{t-1}\right)} \ {o_{t}=\sigma\left(W_{o x} x_{t}+W_{o m} m_{t-1}\right)} \ {c_{t}=f_{t} \cdot c_{t-1}+i_{t} \cdot \tanh \left(W_{c x} x_{t}+W_{c m} m_{t-1}\right)} \ {h_{t}=o_{t} \cdot \tanh \left(c_{t}\right)}\end{array}<br>$$</p><p>上述复杂的公式简写成$h_t^e=LSTM^e(x_t;h_{t-1})$, 其中$e$就表示在隐含层中已经经过了编码。</p><p>解码器就是将编码器的输出当作输入的初始状态，未来的信息也作为输入生成未来序列，也就是输出。解码器设计成一个双向LSTM网络，可以正向和逆向传播未来的输入特征。这个结构可以使得前向和后向的输入都可以在每一个未来的时间步中观察得到。BiLSTM的隐含层传入一个全连接层或者一个时间域上的卷积层来进行最后的预测。我们强调说最后的预测应该是在BiLSTM信息传播后来进行的。通过将信息传播状态和信息预测状态进行分离，防止在一个长视界（long horizon）中错误的累加。这个结构中前向和后向的信息传播是分离的。定义如下：<br>$$<br>\begin{aligned}<br>h_{t}^{f} &amp;=L S T M^{f}\left(x_{t} ; h_{t-1}\right) \<br>h_{t}^{b} &amp;=L S T M^{b}\left(x_{t} ; h_{t+1}\right) \<br>h_{t} &amp;=\left[h_{t}^{f} ; h_{t}^{b}\right]<br>\end{aligned}<br>$$</p><p>上式可以综合写成$h_t^d = BiLSTM^d(x_t;h_{t-1},h_{t+1})$，为了从隐含层生成分位数的预测，我们加入了一个线性层，在编码解码层的K维分位数预测则为<br>$$<br>\begin{aligned}<br>y_{t}^{e} &amp;=W_{e} h_{t}^{e}+b_{e} \<br> y_{t}^{d} &amp;=W_{d} h_{t}^{d}+b_{d}<br> \end{aligned}<br>$$</p><h3 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h3><p>通过Embedding技术可以将明确的变量映射为数个特征向量，采用one-hot编码代表$|C|$变量$x_c={0…010…0}\in R^{|C|}$，学习的嵌入式矩阵$W_c\in R^{D*|C|}$，变化后的变量可以由$x_c’=W_cx_c$计算得。</p><h3 id="注意力模型"><a href="#注意力模型" class="headerlink" title="注意力模型"></a>注意力模型</h3><p>编码解码模型由于内存的更新很难捕获长期的记忆，所以这里使用注意力模型，如图所示<br><img src="pictures/multi-scale.jpg" alt><br>最下层的是历史编码模块，最上层是一个BiLSTM的未来解码模块，文章设计了一个多模型的注意力机制，他由每次未来时间步骤中的BiLSTM的隐藏层引导，并且融合不同的权重应用到不同的历史数据中</p><p><strong>时间注意力</strong></p><p>不能使用全体的历史数据，原因是真实世界的时序数据太长了，可能会模糊掉注意力模型，并且计算不够有效率。文章提出了一种分离性注意每个时期下的历史数据并将其结合到多形态融合的模式下，长度可以由人为设定。时间注意力的权重$\gamma_{1:T_h}$可以表述为<br>$$<br>\begin{array}<br>{c}{\mathbf{g}=\mathbf{v}<em>{g}^{\top} \tanh \left(\mathbf{W}</em>{g} \mathbf{s}<em>{t}+\mathbf{V}</em>{g} \mathbf{h}+\mathbf{b}<em>{g}\right)} \<br>{\gamma</em>{i}=\frac{\exp \left(g_{i}\right)}{\sum_{j=1}^{T_{h}} \exp \left(g_{j}\right)} \quad \text { for } i=1 \ldots T_{h}}<br>\end{array}<br>$$</p><p>然后加入的内容向量$c_t$和transformer$d_t$为：<br>$$<br>\begin{aligned} \mathbf{c}<em>{t} &amp;=\sum</em>{i=1}^{T_{h}} \gamma_{i} \mathbf{h}<em>{i} \ \mathbf{d}</em>{t} &amp;=\operatorname{ReLU}\left(\mathbf{W}<em>{d} \mathbf{c}</em>{t}+\mathbf{b}_{d}\right) \end{aligned}<br>$$</p><p><strong>多形态融合</strong></p><p>图3 给出的是M=2的融合结果，<br>$$<br>\begin{array}{l}{\mathbf{p}<em>{t}^{m}=\mathbf{v}</em>{p}^{\top} \tanh \left(\mathbf{W}<em>{p} \mathbf{s}</em>{t}+\mathbf{V}<em>{p}^{m} \mathbf{d}</em>{t}^{m}+\mathbf{b}<em>{p}\right)} \ {\phi</em>{t}^{m}=\frac{\exp \left(p_{t}^{m}\right)}{\sum_{k=1}^{M} \exp \left(p_{t}^{k}\right)} \quad \text { for } m=1 \ldots M}\end{array}<br>$$</p><p>融合后的知识库$x_t$可以由$d_t^m$的加和计算结果给出</p><p>$$<br>x_t=\sum_{m=1}^M\phi_t^md_t^m<br>$$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Multi-Horizon Time Series Forecasting with Temporal Attention Learning(KDD’ 19)&lt;/p&gt;
&lt;p&gt;文章阐述了历史信息中的时间模式对长时间序列的预测是至关重要的。传统方法中手动设置时间依赖从而探索相关
      
    
    </summary>
    
      <category term="Anomaly detection" scheme="http://www.liufr.com/categories/Anomaly-detection/"/>
    
    
      <category term="Anomaly detection" scheme="http://www.liufr.com/tags/Anomaly-detection/"/>
    
      <category term="time series" scheme="http://www.liufr.com/tags/time-series/"/>
    
  </entry>
  
  <entry>
    <title>多维时序无监督异常检测.md</title>
    <link href="http://www.liufr.com/2019/09/09/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/2019-09-09-DNN%E5%A4%9A%E7%BB%B4%E6%97%B6%E5%BA%8F%E6%97%A0%E7%9B%91%E7%9D%A3%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    <id>http://www.liufr.com/2019/09/09/数据挖掘/2019-09-09-DNN多维时序无监督异常检测/</id>
    <published>2019-09-08T16:00:00.000Z</published>
    <updated>2019-09-09T03:15:11.939Z</updated>
    
    <content type="html"><![CDATA[<p>A Deep Neural Network for Unsupervised Anomaly  Detectionand Diagnosisin Multivariate Time Series Data</p><h2 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h2><ul><li>形式化异常检测到三个子问题上：<ul><li>异常检测</li><li>根因定位</li><li>异常严重性解释</li></ul></li><li>提出了系统签名矩阵（system signature matrix）的概念，用MSCRED通过一个卷积编码器来编码tensor间的联系，基于注意力模型的ConvLSTM网络可以处理时间的模型。该模型首次可以解决上述三个问题</li><li>性能超越baseline</li></ul><h2 id="MSCRED-框架"><a href="#MSCRED-框架" class="headerlink" title="MSCRED 框架"></a>MSCRED 框架</h2><p>Multi-Scale Convolutional Recurrent Encoder-Decoder</p><p>大体思路：</p><ol><li>生成多尺度的系统签名矩阵</li><li>通过卷积编码器把空间信息编码到签名矩阵中</li><li>把时间信息编码到ConvLSTM注意力模型中</li><li>根据卷积解码器重构签名矩阵，利用平方误差来进行端到端的学习</li></ol><h3 id="使用签名矩阵描述状态"><a href="#使用签名矩阵描述状态" class="headerlink" title="使用签名矩阵描述状态"></a>使用签名矩阵描述状态</h3><p>对于时间段$t-w$ 到 $t$ 这段时间内，两个时序$x_i^w =(x_i^{t-w},x_i^{t-w-1},…,x_i^t)$ 与 $x_j^w =(x_j^{t-w},x_j^{t-w-1},…,x_j^t)$，联合计算<br>$$<br>m_{ij}^t = \frac{sum_{\delta=0}^\omega x_i^{t-\delta}x_j^{t-\delta}}{K}<br>$$<br>其中$K$作为缩放因子。这个矩阵可以衡量相似性且有一定的鲁棒性，有$m$构成完全矩阵$M$</p><h3 id="卷积下的编码器"><a href="#卷积下的编码器" class="headerlink" title="卷积下的编码器"></a>卷积下的编码器</h3><p>首先将$M^t$用不同尺度连接成tensor $\chi^{t,0}\in \mathbb{R}^{n<em>n</em>s}$ 然后再传入卷积层，前后两层的通过激活函数连接<br>$$<br>\chi^{t,l}=f(W^l*\chi^{t,l-1}+b^l)<br>$$<br>激励函数用的是SELU</p><p><img src="pictures/MSCRED.jpg" alt></p><h3 id="基于注意力的ConvLSTM"><a href="#基于注意力的ConvLSTM" class="headerlink" title="基于注意力的ConvLSTM"></a>基于注意力的ConvLSTM</h3><p>传统的ConvLSTM可能随着时序序列的增加，性能发生下降，为了解决这个问题，引入了注意力模型，可以根据时间戳动态采取相关隐含层（feature map）。隐含层的更新$H^{t,l} = ConvLSTM(\chi^{t,l},H^{t-1,l})$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;A Deep Neural Network for Unsupervised Anomaly  Detectionand Diagnosisin Multivariate Time Series Data&lt;/p&gt;
&lt;h2 id=&quot;贡献&quot;&gt;&lt;a href=&quot;#贡献&quot; clas
      
    
    </summary>
    
      <category term="Anomaly detection" scheme="http://www.liufr.com/categories/Anomaly-detection/"/>
    
    
      <category term="Anomaly detection" scheme="http://www.liufr.com/tags/Anomaly-detection/"/>
    
  </entry>
  
  <entry>
    <title>异常检测（MSRA，KDD&#39;19）</title>
    <link href="http://www.liufr.com/2019/08/14/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/2019-08-14-%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%EF%BC%88MSRA%20KDD&#39;19%EF%BC%89/"/>
    <id>http://www.liufr.com/2019/08/14/数据挖掘/2019-08-14-异常检测（MSRA KDD&#39;19）/</id>
    <published>2019-08-13T16:00:00.000Z</published>
    <updated>2019-08-15T09:02:54.964Z</updated>
    
    <content type="html"><![CDATA[<p>Time-Series Anomaly Detection Service at Microsoft</p><p>Spectral Residual(SR) 以及 Convolutional Neural Network(CRR) 方法来进行时序异常检测。</p><p>挑战：</p><ol><li>缺少标签：系统往往需要同时处理千百万的数据量，不能手动添加标签。时序数据分布也是不断变化的，需要识别从前没有过的异常模式，也就是说有监督的模型是不可以的。</li><li>普适性：需要兼容多种数据模式。</li><li>有效性：监控系统往往要实时处理很多的数据，尤其是分钟级的数据，异常检测系统需要在有限时间内进行处理。</li></ol><p>CNN是一种带标签的有监督检测，SR则是一个无监督的模型，二者结合。</p><hr><h2 id="系统"><a href="#系统" class="headerlink" title="系统"></a>系统</h2><h3 id="Data-Ingestion"><a href="#Data-Ingestion" class="headerlink" title="Data Ingestion"></a>Data Ingestion</h3><p>连接及粒度。连接可以将用户存储系统和异常检测系统对接，粒度可以确定异常检测任务周期性接纳新的数据。（fluxDB和Kafka）</p><h3 id="Online-Compute"><a href="#Online-Compute" class="headerlink" title="Online Compute"></a>Online Compute</h3><p>在线计算可以让每个数据都在进入流水线后立即得到处理，这里需要一个滑动窗口来优化内存以及保证计算有效性。（Flink）</p><h3 id="Experimentation-Platform"><a href="#Experimentation-Platform" class="headerlink" title="Experimentation Platform"></a>Experimentation Platform</h3><p>衡量性能，可以人工标记一些数据来评判检测的准确性</p><hr><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>异常检测可以使用SR的基础是，时序的异常表现在图形图像上来说往往是突出的。CNN可行的原因是有标签的数据效果较好，因此二者要结合</p><h3 id="SR"><a href="#SR" class="headerlink" title="SR"></a>SR</h3><p>步骤如下：</p><ol><li>快速傅里叶变化FTT得到对数振幅频谱</li><li>计算频谱冗余</li><li>逆傅里叶变化将序列反转到空间域名</li></ol><p>特别的快速傅里叶FFT是通过滑动窗口处理队列的。更多的是关注延迟，发现当检测点是滑动窗口中间点时效果最好，因此要在已知输入的最后一个点后加上几个预测点来从而使该点放在滑动窗口的中心位置，对于$x_n$后的估计点$x_{n+1}$计算如下：<br>$$<br>\bar{g}=\frac{1}{m}\sum_{i=1}^mg(x_n,x_{n-i})\<br>x_{n+1} = x_{n-m+1}+\bar{g}\cdot m<br>$$</p><p>其中$g(x_i,x_j)$可以作为i，j两点直线上的斜率，$\bar{g}$也就是平均梯度，m=5，$x_{n+1}$作为一个关键点这里复制K次加到序列末尾即可。</p><hr><h3 id="SR-CNN"><a href="#SR-CNN" class="headerlink" title="SR-CNN"></a>SR-CNN</h3><p>原始的SR可以通过设置阈值实现异常检测，但是为了解决更复杂的问题要采用CNN，用在显著图（FTT变换后的）而非原始输入图。CNN由两个1-D的卷积层（卷积核大小与滑动窗口大小一样）以及两个全连接层构成，第一个卷积层的通道大小与滑动窗口一样，第二个卷积层大小翻倍，全连接层在Sigmoid输出前是堆叠的，交叉熵采用损失函数</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Time-Series Anomaly Detection Service at Microsoft&lt;/p&gt;
&lt;p&gt;Spectral Residual(SR) 以及 Convolutional Neural Network(CRR) 方法来进行时序异常检测。&lt;/p&gt;
&lt;p&gt;
      
    
    </summary>
    
      <category term="Anomaly detection" scheme="http://www.liufr.com/categories/Anomaly-detection/"/>
    
    
      <category term="Anomaly detection" scheme="http://www.liufr.com/tags/Anomaly-detection/"/>
    
  </entry>
  
  <entry>
    <title>孤立森林（Isolation Forest）</title>
    <link href="http://www.liufr.com/2019/07/21/%E5%AE%89%E5%85%A8/2019-07-22-xgboost/"/>
    <id>http://www.liufr.com/2019/07/21/安全/2019-07-22-xgboost/</id>
    <published>2019-07-20T16:00:00.000Z</published>
    <updated>2019-07-22T02:50:03.805Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="Security" scheme="http://www.liufr.com/categories/Security/"/>
    
    
      <category term="Anomaly detection" scheme="http://www.liufr.com/tags/Anomaly-detection/"/>
    
      <category term="Security" scheme="http://www.liufr.com/tags/Security/"/>
    
  </entry>
  
  <entry>
    <title>实时异常检测（xStream，KDD&#39;18）</title>
    <link href="http://www.liufr.com/2019/07/21/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/2019-08-13-%E5%AE%9E%E6%97%B6%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    <id>http://www.liufr.com/2019/07/21/数据挖掘/2019-08-13-实时异常检测/</id>
    <published>2019-07-20T16:00:00.000Z</published>
    <updated>2019-09-10T13:18:59.518Z</updated>
    
    <content type="html"><![CDATA[<p>xStream: Outlier Dete‘x’ion in Feature-Evolving Data Streams</p><ul><li>首次对evolving数据流进行异常检测，特征：不仅仅由随着时间出现的新的数据点，已有数据点的特征可能会变化也可能会出现新的特征，也就是说数据矩阵的行列都是不确定规模的</li><li>提出算法xStream的特性<ul><li>对于固定数据模型算法计算的内存空间和时间都是固定的；</li><li>使用子空间投影可以处理高维数据；</li><li>多种尺度来衡量异常点，区分离散的异常点和聚集的异常异常群</li><li>基于窗口的方法来处理非平稳数据</li></ul></li></ul><p>问题形式化：网络数据流$D={e_t}_{t=1,2,…}$，每一个元素$e_t$都是一个三元组的形式$(id,f,\delta)_t$。这里的$id$对于每一个数据点来说是唯一的，$f$是特征名，由一个字符串代替，$\delta$是一个连续或者离散的标量。每一个三元组都是对一个不可见特征空间里点的更新。</p><p><strong>问题1: 给定一个数据流$D={e_t}_{t=1,2,…}$以及三元组$(id,f,\delta)_t$计算并持续对每个点打分，要求所有的异常点分数都高于非异常点</strong></p><hr><h2 id="xStream"><a href="#xStream" class="headerlink" title="xStream"></a>xStream</h2><p>通过整合<strong>Half-Space Chains</strong>来有效估计密度，每个链通过大规模计算邻居从而估计某个点的密度</p><ol><li>StreamHash：通过稀疏随机投影来子空间选择来实习降维。</li><li>Half-Space Chains：一种大规模有效密度估计方法。</li><li>扩展到对非静态数据以及进化特征的处理。</li></ol><h3 id="StreamHash"><a href="#StreamHash" class="headerlink" title="StreamHash"></a>StreamHash</h3><p>随机投影。经典的随机投影方法是通过一系列的高斯随机向量${r_1,…,r_K}\subset \mathbb{R}^d$把每一个点$x\in \mathbb{R}^d$投影到一个低维嵌入空间$y\in \mathbb{R}^K$,$y = (x^Tr_1,…,x^Tr_K)$其中K是随机投影的个数，从而保证嵌入向量的距离与原空间的向量距离相似。</p><p>对于高维数据，异常点经常是存在于低维的子空间中，由于不相干特征的存在导致很难检测出异常。这个问题可以通过在已选择的子空间数据中寻找异常来降低困难。改进高斯随机投影为database-friendly random projection，只有其中1/3的向量都是非零向量，在这种规则下，每个投影都忽略2/3的特征空间，始终保持每对点的距离。投影值遵循以下分布：<br>$$<br>r_i[j] = \sqrt{\frac{3}{K}}\left{\begin{matrix}<br>-1 &amp; with\ probability \ \frac{1}{6}\<br>0 &amp; with\ probability \ \frac{2}{3}\<br>+1 &amp; with\ probability \ \frac{1}{6}<br>\end{matrix}\right.<br>$$</p><p>在特征进化的数据流中，真正的维度d是不可观测的，并且数据进化是实时的。因此他不可能抽取一系列随机向量$r_1,…,r_K$作为先验维度。为了解决这个问题，StreamHash是一个基于哈希的离散随机投影，初始化时K个哈希函数$h_1(\cdot),…h_K(\cdot)$,每个哈希函数都能将一个特征名称f映射成一个哈希值$h_i:f\rightarrow \mathbb{R}$,给定一个固定特征空间$\mathcal{F}$的点x，随机投影结果如下：<br>$$<br>y[i]=\sum_{f_j\in\mathcal{F}}h_i(f_j)x[j], i=1,…,K<br>$$<br>上式是对所有特征一次性全部到达的结果</p><p>对于进化特征的数据点，更新的投影id可以通过如下计算：<br>$$<br>y_{id}[i] = y_{id}[i]+h_i(f)\delta, i=1,…,K<br>$$</p><p>如果更新项$y_{id}$不存在，那么他就将使用第一次的更新值作为初始值，在这个投影规则下，尽管数据是在不断接受的，我们仍可以计算并保持低维的投影。</p><p>对于哈希函数，我们使用哈希族$g_1(\cdot),…,g_K(\cdot)$将字符串哈希成32bit整数，令$a_i(f)=g_i(f)/(2^{32}-1)$是一个0~1之间的数字。$h_i(f)$被定义为K随机投影，这一部分其实就是用哈希的方式随机抽取一定的数据进行分布。</p><p>$$<br>h_i[f]=\sqrt{\frac{3}{K}} \left{\begin{matrix}<br>-1 &amp; if\ a_i(f)\in[0,1/6)\<br>0 &amp; if\ a_i(f)\in[1/6,5/6)\<br>+1 &amp; if\ a_i(f)\in[5/6,1]<br>\end{matrix}\right.<br>$$</p><hr><h3 id="Half-Space-Chains"><a href="#Half-Space-Chains" class="headerlink" title="Half-Space Chains"></a>Half-Space Chains</h3><p>密度估计异常点，可以通过半径估计邻居数量。</p><p>两个问题：</p><ol><li>异常检测对于选择的尺度非常敏感</li><li>在高维数据中，随着维数增高其邻居数量逐渐趋近于0</li></ol><p>我们在K维空间上使用密度估计$\mathcal{P}={1,…,K}$,一种方法是通过利用等宽直方图估计，但是随着维数增加，bins也会指数上升，导致每个bins太分散以至于不能可靠地估计密度，我们提出了深度为D的Half-Space Chains方法。每个链都随机在不同level$l=1,…,D$选择一个分割维度$p\in \mathcal{P}$,递归地将高维空间分割成离散的bins。除此之位，如果一个特征在子空间被重复采样，那么将会被离散成一个较小宽度的bin。维度的采样也是通过可替代随机进行采样的。令$\Delta[p]$作为初始bin宽度。初始化bin vector $\bar{z}\in\mathbb{Z}^K$为0，令$p=p[1]\in \mathcal{P}$作为在level=1时的特征采样。bin vector在level=1的bin宽度是$\bar{z}[p]=\left \lfloor y[p]/\Delta[p]  \right \rfloor$,</p><p>对于密度估计来说，bin 的边界设置是至关重要的。这里提出了对每个维度随机偏移$s[p]，s[p]\in Uniform(0,\Delta[p])$。这个偏移减少了确定性边界对聚类结果的影响，因为聚类节点有可能会落入到同一个bin当中。令$o(p,l)$做完特征p在链中采样的次数。详见论文中的例子，有很好解释。<br>$$<br>z[p] = \frac{y[p]+s[p]/2^{o(p,l)-1}}{\Delta[p]/2^{o(p,l)-1}}\<br>\bar{z} = \left \lfloor z \right \rfloor<br>$$</p><p>打分：<br>$$<br>S(y)=\frac{1}{M}\sum_{c\in C}S_{C}(y)=\frac{1}{M}\sum_{c\in C}min_l 2^lH_l[\bar{z}]<br>$$<br>对于给出的有异常的图$S(x)$,输出序列如下：<br>$$<br>O(x_i) = \left{\begin{matrix}<br>1, if\ \frac{S(x_i)-\bar{S(x_i)}}{\bar{S(x_i)}} &gt; \tau \<br>0, otherwise<br>\end{matrix}\right.<br>$$</p><p>其中x是序列中任意一个点，$S(x_i)$是异常图中对应的点，$\bar{S(x_i)}$是当前平均点</p><p>通过上述计算可以得到任意level的$\bar{z}$，我们可以索引其到计数的数据结构中，以得到该level的一个bin-count。然而，独一无二的bin vectors可能会在无限的网络流场景中特别大，因此我们在不同的level上使用一个count-min-sketch，$H_l\in H,l=1,…,D$,可以通过这个高精确度地估计bin-counts。</p><p>总的来说，每个链可以定义为$C={p,\Delta,s,H}$,这样链的集合可以构成一个$C ={C_1,…C_M}$,从而组成了一个Half-Space Chains</p><p>接下来就是<strong>构建适当的bin-vecto</strong>，通过上面的公式我们可以在不同level $l=1,…D$ 计算bin-vector，然而，如下的递归式可以增量地计算非离散化的bin vectors， 从$l=1$开始，向下层逐步计算算术操作。</p><p>$$<br>z[p] = (y[p]+s[p])/\Delta [p] \ \ if\ o(p,l)=1 \<br>z[p] = 2z[p] - s[p]/\Delta[p] \ \ if\ o(p,l)&gt;1<br>$$</p><p>其中p是在level $l$ 处的特征采样，$o(p,l)$ 表示该特征曾经被采样过的次数</p><p><strong>在Half-Space Chain 中更新bin-counts</strong> ，bin-vector可以索引到$H_l$中来进行bin-count计数$H_l[\bar{z}]$,但存在情况要把链中的点删除，当某个点（特征）发生进化，就需要<strong>删除原投影，并加入更新后的投影</strong>，删除过程与加入过程类似，bin-count要使用递减的方式来代替。</p><p><strong>多尺度的异常打分</strong>。在更高的level，由于维度关系，bin-counts可能会趋近于非常低的数，为了针对不同的level都有合理的分数，推断bin-count为$2^lH_l[\bar{z}]$,这和当初始bin宽度$\Delta[p]$恰巧等于p范围的一半时，期望的每条链中的随机分布相等。</p><p>我们定义的多尺度异常评分</p><p>$$<br>S(y)=\frac{1}{M}\sum_{C\in C}S_C(y)=\frac{1}{M}\sum_{C\in C}min_l2^lH_k[\bar{z}]<br>$$</p><p>更新bin-counts与计算异常点的评分时同步的。</p><h3 id="处理非静态数据"><a href="#处理非静态数据" class="headerlink" title="处理非静态数据"></a>处理非静态数据</h3><p>问题的关键在于随着网络流的变化，数据分布也会变化，导致bin-counts的构成不再能代表现在数据的分布。</p><p><strong>处理非静态数据</strong> 使用窗口，分为当前窗口和参考窗口，当点y开始在链中传播时，用参考窗口计算评分，当前窗口进行更新，当出现新的点，用当前窗口代替参考窗口，并把当前窗口初始化为0。窗口有分布的频率确定，高频适合小窗口，但是也不能太小，会导致变化过大。</p><p><strong>特征进化的点</strong> 前面的公式介绍了计算方法，实际要保留固定cache的N个投影点，采用LRU算法保留特征</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;xStream: Outlier Dete‘x’ion in Feature-Evolving Data Streams&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;首次对evolving数据流进行异常检测，特征：不仅仅由随着时间出现的新的数据点，已有数据点的特征可能会变化也可能会出现新的特
      
    
    </summary>
    
      <category term="Anomaly detection" scheme="http://www.liufr.com/categories/Anomaly-detection/"/>
    
    
      <category term="Anomaly detection" scheme="http://www.liufr.com/tags/Anomaly-detection/"/>
    
  </entry>
  
  <entry>
    <title>多项式插值</title>
    <link href="http://www.liufr.com/2019/07/21/%E5%AE%89%E5%85%A8/2019-07-21-%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%8F%92%E5%80%BC/"/>
    <id>http://www.liufr.com/2019/07/21/安全/2019-07-21-多项式插值/</id>
    <published>2019-07-20T16:00:00.000Z</published>
    <updated>2019-07-21T13:36:30.148Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Polynominal-Interpolation"><a href="#Polynominal-Interpolation" class="headerlink" title="Polynominal Interpolation"></a>Polynominal Interpolation</h2><p>多项式插值，给定一组数据，寻找一个恰好通过这些数据点的多项式。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> Ridge</span><br><span class="line"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> PolynomialFeatures</span><br><span class="line"><span class="hljs-keyword">from</span> sklearn.pipeline <span class="hljs-keyword">import</span> make_pipeline</span><br><span class="line"><span class="hljs-keyword">from</span> time_series_detector.common.tsd_common <span class="hljs-keyword">import</span> *</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PolynomialInterpolation</span><span class="hljs-params">(object)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    In statistics, polynomial regression is a form of regression analysis in which the relationship</span></span><br><span class="line"><span class="hljs-string">    between the independent variable x and the dependent variable y is modelled as an nth degree polynomial in x.</span></span><br><span class="line"><span class="hljs-string">    WIKIPEDIA: https://en.wikipedia.org/wiki/Polynomial_regression</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, threshold=<span class="hljs-number">0.15</span>, degree=<span class="hljs-number">4</span>)</span>:</span></span><br><span class="line">        <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">       :param threshold: The critical point of normal.</span></span><br><span class="line"><span class="hljs-string">       :param degree: Depth of iteration.</span></span><br><span class="line"><span class="hljs-string">        """</span></span><br><span class="line">        self.degree = degree</span><br><span class="line">        self.threshold = threshold</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(self, X, window=DEFAULT_WINDOW)</span>:</span></span><br><span class="line">        <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">        Predict if a particular sample is an outlier or not.</span></span><br><span class="line"><span class="hljs-string">        :param X: the time series to detect of</span></span><br><span class="line"><span class="hljs-string">        :param type X: pandas.Series</span></span><br><span class="line"><span class="hljs-string">        :param window: the length of window</span></span><br><span class="line"><span class="hljs-string">        :param type window: int</span></span><br><span class="line"><span class="hljs-string">        :return: 1 denotes normal, 0 denotes abnormal</span></span><br><span class="line"><span class="hljs-string">        """</span></span><br><span class="line">        x_train = list(range(<span class="hljs-number">0</span>, <span class="hljs-number">2</span> * window + <span class="hljs-number">1</span>)) + list(range(<span class="hljs-number">0</span>, <span class="hljs-number">2</span> * window + <span class="hljs-number">1</span>)) + list(range(<span class="hljs-number">0</span>, window + <span class="hljs-number">1</span>))</span><br><span class="line">        x_train = np.array(x_train)</span><br><span class="line">        x_train = x_train[:, np.newaxis]</span><br><span class="line">        avg_value = np.mean(X[-(window + <span class="hljs-number">1</span>):])</span><br><span class="line">        <span class="hljs-keyword">if</span> avg_value &gt; <span class="hljs-number">1</span>:</span><br><span class="line">            y_train = X / avg_value</span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            y_train = X</span><br><span class="line">        model = make_pipeline(PolynomialFeatures(self.degree), Ridge())</span><br><span class="line">        model.fit(x_train, y_train)</span><br><span class="line">        <span class="hljs-keyword">if</span> abs(y_train[<span class="hljs-number">-1</span>] - model.predict(np.array(x_train[<span class="hljs-number">-1</span>]).reshape(<span class="hljs-number">1</span>, <span class="hljs-number">-1</span>))) &gt; self.threshold:</span><br><span class="line">            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span></span><br><span class="line">        <span class="hljs-keyword">return</span> <span class="hljs-number">1</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Polynominal-Interpolation&quot;&gt;&lt;a href=&quot;#Polynominal-Interpolation&quot; class=&quot;headerlink&quot; title=&quot;Polynominal Interpolation&quot;&gt;&lt;/a&gt;Polynominal
      
    
    </summary>
    
      <category term="Security" scheme="http://www.liufr.com/categories/Security/"/>
    
    
      <category term="Anomaly detection" scheme="http://www.liufr.com/tags/Anomaly-detection/"/>
    
      <category term="Security" scheme="http://www.liufr.com/tags/Security/"/>
    
  </entry>
  
  <entry>
    <title>指数加权移动平均法（EWMA）</title>
    <link href="http://www.liufr.com/2019/07/21/%E5%AE%89%E5%85%A8/2019-07-21-%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E7%A7%BB%E5%8A%A8%E5%B9%B3%E5%9D%87%E6%B3%95%EF%BC%88EWMA%EF%BC%89/"/>
    <id>http://www.liufr.com/2019/07/21/安全/2019-07-21-指数加权移动平均法（EWMA）/</id>
    <published>2019-07-20T16:00:00.000Z</published>
    <updated>2019-07-21T11:10:52.001Z</updated>
    
    <content type="html"><![CDATA[<h2 id="主要思想"><a href="#主要思想" class="headerlink" title="主要思想"></a>主要思想</h2><p>Exponentially Weighted Moving Average (EWMA)，对观察值给予不同的权重求得平均值，并以平均值为基础确定预测值的方法，往往近期观察值的权重高是因为其更能反映近期变化的趋势。各个指数加权系数是随时间指数递减的，越靠近当前时刻，加权系数就越大。</p><p>可以视作一种理想的最大似然估计，当前估计值由前一次估计值和当前的抽样值共同决定；也可以视作一个低通滤波器，剔除短期波动保留长期发展趋势的平滑形式。</p><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ol><li>不需要保存过去的所有数值</li><li>计算量显著减小</li></ol><h2 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h2><p>$$ v_t = \beta v_{t-1}+(1-\beta)\theta_t$$</p><p>其中$\theta_t$为时刻$t$的实际温度；$\beta$表示加权下降的速率，其值越小下降越快；$v_t$为$t$时刻的EWMA值。</p><h2 id="变差修正"><a href="#变差修正" class="headerlink" title="变差修正"></a>变差修正</h2><p>如果初始化$v_0=0$,那么初期值都会偏小，虽然最后会慢慢减小这部分影响但是还是对公式做出适当修正：<br>$$v_t = \frac{\beta v_{t-1}+(1-\beta)\theta_t}{1-\beta^t}$$</p><p>当t表较小（初期）分母可以很好放大当前数值，当t很大，分母趋为1，对数值机会没有影响。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Ewma</span><span class="hljs-params">(object)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    In statistical quality control, the EWMA chart (or exponentially weighted moving average chart)</span></span><br><span class="line"><span class="hljs-string">    is a type of control chart used to monitor either variables or attributes-type data using the monitored business</span></span><br><span class="line"><span class="hljs-string">    or industrial process's entire history of output. While other control charts treat rational subgroups of samples</span></span><br><span class="line"><span class="hljs-string">    individually, the EWMA chart tracks the exponentially-weighted moving average of all prior sample means.</span></span><br><span class="line"><span class="hljs-string">    WIKIPEDIA: https://en.wikipedia.org/wiki/EWMA_chart</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, alpha=<span class="hljs-number">0.3</span>, coefficient=<span class="hljs-number">3</span>)</span>:</span></span><br><span class="line">        <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">        :param alpha: Discount rate of ewma, usually in (0.2, 0.3).</span></span><br><span class="line"><span class="hljs-string">        :param coefficient: Coefficient is the width of the control limits, usually in (2.7, 3.0).</span></span><br><span class="line"><span class="hljs-string">        """</span></span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        self.coefficient = coefficient</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(self, X)</span>:</span></span><br><span class="line">        <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">        Predict if a particular sample is an outlier or not.</span></span><br><span class="line"><span class="hljs-string">        :param X: the time series to detect of</span></span><br><span class="line"><span class="hljs-string">        :param type X: pandas.Series</span></span><br><span class="line"><span class="hljs-string">        :return: 1 denotes normal, 0 denotes abnormal</span></span><br><span class="line"><span class="hljs-string">        """</span></span><br><span class="line">        s = [X[<span class="hljs-number">0</span>]]</span><br><span class="line">        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, len(X)):</span><br><span class="line">            temp = self.alpha * X[i] + (<span class="hljs-number">1</span> - self.alpha) * s[<span class="hljs-number">-1</span>]</span><br><span class="line">            s.append(temp)</span><br><span class="line">        s_avg = np.mean(s)</span><br><span class="line">        sigma = np.sqrt(np.var(X))</span><br><span class="line">        ucl = s_avg + self.coefficient * sigma * np.sqrt(self.alpha / (<span class="hljs-number">2</span> - self.alpha))</span><br><span class="line">        lcl = s_avg - self.coefficient * sigma * np.sqrt(self.alpha / (<span class="hljs-number">2</span> - self.alpha))</span><br><span class="line">        <span class="hljs-keyword">if</span> s[<span class="hljs-number">-1</span>] &gt; ucl <span class="hljs-keyword">or</span> s[<span class="hljs-number">-1</span>] &lt; lcl:</span><br><span class="line">            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span></span><br><span class="line">        <span class="hljs-keyword">return</span> <span class="hljs-number">1</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;主要思想&quot;&gt;&lt;a href=&quot;#主要思想&quot; class=&quot;headerlink&quot; title=&quot;主要思想&quot;&gt;&lt;/a&gt;主要思想&lt;/h2&gt;&lt;p&gt;Exponentially Weighted Moving Average (EWMA)，对观察值给予不同的权重求得平均值，
      
    
    </summary>
    
      <category term="Security" scheme="http://www.liufr.com/categories/Security/"/>
    
    
      <category term="Anomaly detection" scheme="http://www.liufr.com/tags/Anomaly-detection/"/>
    
      <category term="Security" scheme="http://www.liufr.com/tags/Security/"/>
    
  </entry>
  
  <entry>
    <title>孤立森林（Isolation Forest）</title>
    <link href="http://www.liufr.com/2019/07/21/%E5%AE%89%E5%85%A8/2019-07-22-%E5%AD%A4%E7%AB%8B%E6%A3%AE%E6%9E%97%EF%BC%88Isolation%20Froest%EF%BC%89/"/>
    <id>http://www.liufr.com/2019/07/21/安全/2019-07-22-孤立森林（Isolation Froest）/</id>
    <published>2019-07-20T16:00:00.000Z</published>
    <updated>2019-07-22T02:39:55.715Z</updated>
    
    <content type="html"><![CDATA[<h2 id="iForest"><a href="#iForest" class="headerlink" title="iForest"></a>iForest</h2><p>无参数无监督方法。</p><p>假设用一个随机超平面切割数据空间，切一次产生两个子空间，依次循环下去直到每个子空间只有一个数据点为止。直观的说那些密度很高的簇需要被切割很多次才会停止，但是那些密度较低的点很容易较早地停在一个子空间里</p><h2 id="实现步骤"><a href="#实现步骤" class="headerlink" title="实现步骤"></a>实现步骤</h2><p>切割是随机的因此需要集成方法得到一个收敛值，即反复从头开始切然后平均每次的结果。</p><ol><li>从训练数据集中选择样本点作为subsample,放入树的根节点。</li><li>随机指定一个维度，随机产生一个切割点p，位于当前数据的最大最小值之间。</li><li>以切割点生成超平面，然后将当前的节点数据空间划分为2个子空间，把小于p的放入节点的左孩子，大于等于p的放在右孩子</li><li>在孩子节点中不断递归步骤2和3，直到无法切割，或者达到限定高度。</li></ol><p>获得t个iTree后，训练结束然后可以用生成的iForest来评估测试数据，对于每一个训练数据x遍历每一棵iTree，计算其落在第几层，得到x在每个树高度平均值。获得测试数据的高度平均值后可以设置一个阈值，低于阈值的测试数据为异常。异常在书中往往有较短的平均高度。论文对高度进行归一化结果是一个0到1的数值，越短高度越接近1（异常可能性越高）</p><p><img src="pictures/iForest.webp" alt></p><p>可以看出d最有可能是异常</p><h2 id="默认参数"><a href="#默认参数" class="headerlink" title="默认参数"></a>默认参数</h2><ul><li>subsample size：256</li><li>Tree height：8</li><li>Number of trees：100</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;iForest&quot;&gt;&lt;a href=&quot;#iForest&quot; class=&quot;headerlink&quot; title=&quot;iForest&quot;&gt;&lt;/a&gt;iForest&lt;/h2&gt;&lt;p&gt;无参数无监督方法。&lt;/p&gt;
&lt;p&gt;假设用一个随机超平面切割数据空间，切一次产生两个子空间，依次循环下
      
    
    </summary>
    
      <category term="Security" scheme="http://www.liufr.com/categories/Security/"/>
    
    
      <category term="Anomaly detection" scheme="http://www.liufr.com/tags/Anomaly-detection/"/>
    
      <category term="Security" scheme="http://www.liufr.com/tags/Security/"/>
    
  </entry>
  
  <entry>
    <title>利用时序决策树进行DGA僵尸检测</title>
    <link href="http://www.liufr.com/2019/07/21/%E5%AE%89%E5%85%A8/2019-07-22-%E5%88%A9%E7%94%A8%E6%97%B6%E5%BA%8F%E5%86%B3%E7%AD%96%E6%A0%91%E8%BF%9B%E8%A1%8CDGA%E5%83%B5%E5%B0%B8%E6%A3%80%E6%B5%8B/"/>
    <id>http://www.liufr.com/2019/07/21/安全/2019-07-22-利用时序决策树进行DGA僵尸检测/</id>
    <published>2019-07-20T16:00:00.000Z</published>
    <updated>2019-07-22T07:12:27.904Z</updated>
    
    <content type="html"><![CDATA[<p>DGA Bot Detection with Time Series Decision Trees</p><h2 id="主要思路"><a href="#主要思路" class="headerlink" title="主要思路"></a>主要思路</h2><ol><li>标签模块依赖分类技术，该模块可以对训练模型生成有标签的数据，并对每一个已经检测到的僵尸网络提供一系列的感染IP</li><li>训练模块使用对生成的标签数据进行有监督的学习，对每一个僵尸网络家族都构造一个检测模型。利用典型的IP时间轮廓评估。</li><li>检测模块在短时间内检测受到影响的IP。</li></ol><h2 id="标签模块"><a href="#标签模块" class="headerlink" title="标签模块"></a>标签模块</h2><p>给有监督训练模型提供受感染IP的实例，该模块只处理在DNS服务器中不存在的域名，利用的原理是通过将相同DGA僵尸网络的子集请求进行聚类。</p><ol><li><strong>数据预处理</strong> 通过启发式方法减小数据量，减少良性的DNS流量，只保留可疑的NXDOMAIN来进行后续的聚类。（根据域名进行判断）</li><li><strong>分级聚类</strong> D作为一系列的域名，每一个域名d都由一组IP代表，这组IP可以至少区分一个不成功的DNS请求。聚类结果C可以代表D的一种划分，对于每一个类别c都是自底向上聚类的结果。两个类可以计算相似度，如果相似度在限制内可以进行合并；该限制也可以防止没有共享足够IP的两个类进行合并。</li></ol><p>相似度计算公式：<br>$$<br>\forall(c_i,c_j) \in C^2, sim(c_i.c_j) = |IP^{c_i}\cap IP^{c_j} |<br>$$</p><p>相似度限制公式：<br>$$<br>sim(c_1^<em>,c_2^</em>) &gt; \alpha \cdot min(|IP^{c_1^<em>}|,|IP^{c_2^</em>}|)<br>$$</p><p><img src="pictures/DGA.jpg" alt></p><ol start="3"><li><strong>得到受感染的IP</strong> 往往丢弃与恶意行为不相干的类别，对已识别的DGA僵尸网络命名，</li></ol><h2 id="检测模块：时序决策树"><a href="#检测模块：时序决策树" class="headerlink" title="检测模块：时序决策树"></a>检测模块：时序决策树</h2><p>输入：DNS时序，每个IP都有标签{Infected，NonInfected}</p><p>在CART决策树的基础上训练，根节点包含所有的训练实例。每个内部节点都包含一系列的训练实例$\tau$以及根据输入特征确定的分割条件s</p><ol><li>所有的实例拥有相同的标签<br>$$\forall(T_0,T_1) \in \tau^2, label(T_0)=label(T_1)$$</li><li>所有的实例根据距离公式都是相等的<br>$$\forall(T_0,T_1) \in \tau^2, dist(T_0,T_1)=0$$</li></ol><h3 id="训练模块"><a href="#训练模块" class="headerlink" title="训练模块"></a>训练模块</h3><p>分治算法进行决策树的处理，一条子树持续划分直到满足上述两个限制。</p><p>对于一个训练集，首先计算分裂的候选值，要求分裂结果使得子树最干净，干净可以用标签的定义来进行判断。最不干净的结果就是一半为{Infected}另一半为{NonInfected}</p><h3 id="检测模块"><a href="#检测模块" class="headerlink" title="检测模块"></a>检测模块</h3><p>当决策树构建结束后，就可以通过从根节点到叶子节点的一条路径上的划分来区分其属于哪一个类。</p><h3 id="时序划分候选"><a href="#时序划分候选" class="headerlink" title="时序划分候选"></a>时序划分候选</h3><ul><li>标准划分：通过半径划分</li><li>聚类划分：根据到中心的的距离进行划分</li></ul><p><img src="pictures/DGA1.jpg" alt></p><h3 id="时序距离"><a href="#时序距离" class="headerlink" title="时序距离"></a>时序距离</h3><p>DTW距离（很好的分析时序距离的方法）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;DGA Bot Detection with Time Series Decision Trees&lt;/p&gt;
&lt;h2 id=&quot;主要思路&quot;&gt;&lt;a href=&quot;#主要思路&quot; class=&quot;headerlink&quot; title=&quot;主要思路&quot;&gt;&lt;/a&gt;主要思路&lt;/h2&gt;&lt;ol&gt;
&lt;li
      
    
    </summary>
    
      <category term="Security" scheme="http://www.liufr.com/categories/Security/"/>
    
    
      <category term="Anomaly detection" scheme="http://www.liufr.com/tags/Anomaly-detection/"/>
    
      <category term="Security" scheme="http://www.liufr.com/tags/Security/"/>
    
  </entry>
  
  <entry>
    <title>梯度提升决策树（GBDT）</title>
    <link href="http://www.liufr.com/2019/07/21/%E5%AE%89%E5%85%A8/2019-07-21-%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%86%B3%E7%AD%96%E6%A0%91(GBDT)/"/>
    <id>http://www.liufr.com/2019/07/21/安全/2019-07-21-梯度提升决策树(GBDT)/</id>
    <published>2019-07-20T16:00:00.000Z</published>
    <updated>2019-07-22T02:02:12.151Z</updated>
    
    <content type="html"><![CDATA[<h2 id="GCDT：梯度提升决策树"><a href="#GCDT：梯度提升决策树" class="headerlink" title="GCDT：梯度提升决策树"></a>GCDT：梯度提升决策树</h2><p>Gradient Boosting Decision Tree，又叫做MART（Multiple Additive Regression Tree）迭代的决策树算法，由多颗决策树组成，具有较强的泛化能力。GBDT是一回归树用来做回归预测，调整后可以用于分类。</p><h2 id="Regression-Decision-Tree：回归树"><a href="#Regression-Decision-Tree：回归树" class="headerlink" title="Regression Decision Tree：回归树"></a>Regression Decision Tree：回归树</h2><p>回归树的每一个节点都会有一个预测值（例如平均值），分支时穷举每一个feature的每个阈值找到最好的分割点，且用最小化平方误差进行衡量分类标准。也就是说分类错误越多平方误差越大，直到每个叶子节点上的预测值都能达到预设的终止条件。</p><h2 id="Boosting-Decision-Tree：提升树算法"><a href="#Boosting-Decision-Tree：提升树算法" class="headerlink" title="Boosting Decision Tree：提升树算法"></a>Boosting Decision Tree：提升树算法</h2><p>Boosting：将弱学习提升为强学习器的算法。主要思想：对于负责任务，将多个专家的判断进行适当综合得出对应的判断，比任何一个专家单独的判断要好。</p><p>提升树通过迭代多个回归树来进行共同决策。当采用平方误差损失函数，每一个回归树学习的是所有树的结论和残差，拟合得到当前的残差回归树。残差=真实值-预测值。提升树可以视作整个迭代过程生成的回归树的累加。</p><p>每一次迭代都增加被错误分类的样本的权重，使模型在之后的迭代中更加注意到难以分类的样本。</p><p><img src="pictures/GBDT.webp" alt></p><h2 id="Xgboost-和-GBDT-的区别："><a href="#Xgboost-和-GBDT-的区别：" class="headerlink" title="Xgboost 和 GBDT 的区别："></a>Xgboost 和 GBDT 的区别：</h2><h3 id="GBDT："><a href="#GBDT：" class="headerlink" title="GBDT："></a>GBDT：</h3><ol><li>GBDT 它的非线性变换比较多，表达能力强，而且不需要做复杂的特征工程和特征变换。</li><li>GBDT 的缺点也很明显，Boost 是一个串行过程，不好并行化，而且计算复杂度高，同时不太适合高维稀疏特征；</li><li>传统 GBDT 在优化时只用到一阶导数信息。</li></ol><h3 id="Xgboost："><a href="#Xgboost：" class="headerlink" title="Xgboost："></a>Xgboost：</h3><ol><li>显示的把树模型复杂度作为正则项加到优化目标中。</li><li>公式推导中用到了二阶导数，用了二阶泰勒展开。（GBDT 用牛顿法貌似也是二阶信息）</li><li>实现了分裂点寻找近似算法。</li><li>利用了特征的稀疏性。</li><li>数据事先排序并且以 block 形式存储，有利于并行计算。</li><li>基于分布式通信框架 rabit，可以运行在 MPI 和 yarn 上。（最新已经不基于 rabit 了）</li><li>实现做了面向体系结构的优化，针对 cache 和内存做了性能优化</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;GCDT：梯度提升决策树&quot;&gt;&lt;a href=&quot;#GCDT：梯度提升决策树&quot; class=&quot;headerlink&quot; title=&quot;GCDT：梯度提升决策树&quot;&gt;&lt;/a&gt;GCDT：梯度提升决策树&lt;/h2&gt;&lt;p&gt;Gradient Boosting Decision Tre
      
    
    </summary>
    
      <category term="Security" scheme="http://www.liufr.com/categories/Security/"/>
    
    
      <category term="Anomaly detection" scheme="http://www.liufr.com/tags/Anomaly-detection/"/>
    
      <category term="Security" scheme="http://www.liufr.com/tags/Security/"/>
    
  </entry>
  
  <entry>
    <title>网络异常行为检测</title>
    <link href="http://www.liufr.com/2019/06/22/%E5%AE%89%E5%85%A8/2019-06-22-%E7%BD%91%E7%BB%9C%E5%BC%82%E5%B8%B8%E8%A1%8C%E4%B8%BA%E6%A3%80%E6%B5%8B/"/>
    <id>http://www.liufr.com/2019/06/22/安全/2019-06-22-网络异常行为检测/</id>
    <published>2019-06-21T16:00:00.000Z</published>
    <updated>2019-06-27T08:20:18.197Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基于统计学习的网络异常行为检测技术-启明"><a href="#基于统计学习的网络异常行为检测技术-启明" class="headerlink" title="基于统计学习的网络异常行为检测技术(启明)"></a><a href="http://www.199it.com/archives/436132.html" target="_blank" rel="noopener">基于统计学习的网络异常行为检测技术</a>(启明)</h2><p>基于人工特征提取的异常检测技术的技术路线是：分析人员首先以某种方式从原始数据中提取特征参数，然后基于特征进行建模和异常检测。</p><p>人工特征提取的优点是（应用较多）：</p><ul><li>特征提取建立在安全分析人员的认知基础之上，对异常行为有较强的针对性；</li><li>对训练样本数量的依赖度低，较少的样本训练即可得到相对准确的模型；</li><li>模型的可解释度高，容易确定异常检测结果的有效性。</li></ul><p>人工特征提取的缺点：</p><ul><li>对安全分析人员的依赖度高，特征的选取方法会对异常检测结果的有效性产生直接影响。</li></ul><p>深度学习优点：</p><ul><li>提出了一种让计算机自动学习产生特征的方法，并将特征学习融入建立模型的过程中，从而减少了人为设计特征引发的不完备。</li></ul><p>深度学习方法局限性：</p><ul><li>需要有大量的训练样本进行训练，才能保证模型的准确度。</li><li>当训练样本数量不足时，深度学习算法将不能够对数据的规律进行无偏估计，模型的识别效果可能还不如传统基于人工特征提取的统计分析方法。</li><li>当前深度学习的成功应用大都集中在有大量训练样本的模式识别领域，如语音识别、图像识别、机器翻译等。</li></ul><p>特征选择：</p><ul><li>命令与控制通道行为检测<ul><li>反向连接特征（判断是不是一个流）</li><li>心跳特征（检测数据传输的平稳度）</li></ul></li><li>获取行为检测<ul><li>会话信息类指标。统计一台主机单位时间内不同协议类型的会话统计信息。如TCP连接次数、UDP连接次数、对应的流量、数据分组大小的均值和标准差等</li><li>应用分布类指标。统计一台主机单位时间内不同应用类型的访问统计信息。如访问不同应用的流量分布、次数、目标地址位置、国别分布等</li><li>指示位标识类指标。统计一台主机单位时间内收发的含特定协议标识位的数据分组数量及其比值。有TCP_SYN_send、TCP_SYN_ACK_receive、RST_send等标志位的会话数目；TCP_SYN_ACK_receive/TCP_SYN_send的比值；单位时间里的ICMP_T3、ICMP_Echo_Reply、ICMP_Echo_Request等报文数目</li><li>地址分布指标。统计一台主机单位时间内访问的IP地址网段分布、内外网分布等参数</li></ul></li></ul><p>特征值异常检测</p><hr><h2 id="基于主动学习的异常检测"><a href="#基于主动学习的异常检测" class="headerlink" title="基于主动学习的异常检测"></a><a href="https://mp.weixin.qq.com/s?__biz=MzIyMzcxMDYwNg==&mid=2247484232&idx=1&sn=28dcd0f1d32ab1298f2ba9c5380d1ce7&chksm=e81b5fa9df6cd6bf391a188c31cb1ac53024f0fd1b6fddb48b13e04882be5d13b53b30eba482&mpshare=1&scene=1&srcid=#rd" target="_blank" rel="noopener">基于主动学习的异常检测</a></h2><p>运维人员会把自己的领域知识融入到异常检测中，针对初始的异常分数排名，给出排名靠前的样本的反馈之后，模型会根据这个反馈来重新调整参数，从而输出新的异常分数排名，目的是使运维人员感兴趣的异常排在靠前的位置，从而节约排查异常所需的时间。</p><p><strong>损失函数</strong>：<br>异常样本被排在最前面，说明目前的模型是好的，损失函数的值需要减少,正常点排在前面，那么这个模型是有问题的，需要增大损失函数。</p><p><strong>镜像下降学习算法</strong></p><hr><h2 id="通过半监督学习为新KPI曲线快速部署异常检测"><a href="#通过半监督学习为新KPI曲线快速部署异常检测" class="headerlink" title="通过半监督学习为新KPI曲线快速部署异常检测"></a><a href="https://mp.weixin.qq.com/s?__biz=MzIyMzcxMDYwNg==&mid=2247484321&idx=1&sn=2d7e8fa47adca709814242fa0308c074&chksm=e81b5f40df6cd656ef182ab30c3a7d117268e21e2823d5ab2be438478b5843039b8eb03aca84&mpshare=1&scene=1&srcid=#rd" target="_blank" rel="noopener">通过半监督学习为新KPI曲线快速部署异常检测</a></h2><p>分析KPI曲线，尽管KPI曲线的数量众多，但是曲线的形状大体上只有少数的几类。这本质是由于，曲线的形状是由业务的类型（比如交易量，游戏在线人数，搜索）以及曲线的类型（CPU使用率，成功率）所决定的，而非集群的规模。除非是上线一个全新的业务，否则新的曲线的形状大概率会和旧曲线的形状相似。这启示作者，可以对曲线做聚类，属于同一类别的曲线，其特征空间也大致相似。属于同一类别的曲线形状相似，但是并不完全一致，在模型层面上需要有所区分。作者开始思考，是否可以用机器学习中的半监督算法来解决这个问题，既可以使用原有的label标注，提高准确率，又能够为每根新上线的曲线分配不同的模型。</p><p><a href="https://zhuanlan.zhihu.com/p/50698719" target="_blank" rel="noopener">ROCKA</a> 分析曲线相似度</p><hr><p><a href="https://github.com/Tencent/Metis" target="_blank" rel="noopener">腾讯Metis</a></p><ol><li>对于已知的故障：织云 Metis 能够综合故障数据和人工经验自动提取故障特征，以故障特征库的形式，自动匹配定位故障；</li><li>对于未知场景：织云 Metis 可根据故障特征推算出可能的原因，并在人工确认后加入故障特征库。</li></ol><p>特征：提供三类时间序列的特征（统计特征、拟合特征、分类特征）用于对时序数据进行特征提取，在监督学习和训练中使用。支持增加自定义特征</p><p>算法： 提供常见的几种机器学习算法封装（统计判别算法、指数移动平均算法、多项式算法、GBDT和xgboost等）用于对序数据进行联合仲裁检测。</p><hr><p><a href="https://mp.weixin.qq.com/s/AXhjawsINKl6cLDV1yf6fw" target="_blank" rel="noopener">百度</a></p><ol><li>阈值检测</li><li>突升突降类算法（检测均值漂移）</li><li>同比类算法（判断正态分布的均值与方差）</li><li>参数自动配置</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;基于统计学习的网络异常行为检测技术-启明&quot;&gt;&lt;a href=&quot;#基于统计学习的网络异常行为检测技术-启明&quot; class=&quot;headerlink&quot; title=&quot;基于统计学习的网络异常行为检测技术(启明)&quot;&gt;&lt;/a&gt;&lt;a href=&quot;http://www.199it
      
    
    </summary>
    
      <category term="Security" scheme="http://www.liufr.com/categories/Security/"/>
    
    
      <category term="Security" scheme="http://www.liufr.com/tags/Security/"/>
    
  </entry>
  
  <entry>
    <title>大数据存储系统</title>
    <link href="http://www.liufr.com/2019/06/04/%E6%95%B0%E6%8D%AE%E5%BA%93/2019-06-04-%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%BF%90%E7%AE%97/"/>
    <id>http://www.liufr.com/2019/06/04/数据库/2019-06-04-大数据运算/</id>
    <published>2019-06-03T16:00:00.000Z</published>
    <updated>2019-06-05T02:42:34.192Z</updated>
    
    <content type="html"><![CDATA[<h2 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h2><p>程序员写串行程序，系统并行分布式执行。</p><p>数据模型&lt;key,value&gt;</p><ul><li>数据由一条一条记录组成</li><li>记录之间无序</li><li>每一条记录有一个key，一个value</li><li>key可以不唯一</li><li>key与value具体类型和内部结构由程序员决定</li></ul><ol><li>Map(ik,iv)-&gt;{&lt;mk,mv&gt;}</li><li>Shuffle = group by mk, 对所有的map输出进行group by，相同mk的所有mv提供给Reduce</li><li>Reduce(mk,{mv})-&gt;{&lt;ok,ov&gt;}</li></ol><p>JobTracker/Name Node控制协调作业的运行；TaskTracker/Data Node执行Map和Reduce的任务。</p><p>MR运行：</p><ol><li>提交作业：Map/Reduce函数，配置信息，输入输出路径。</li><li>Map Task读数据：Split对应一个Map Task数据块个数可能多余Mapper个数，每个Mapper可能处理多个Task，就近处理。</li><li>Map Task执行：对于产生的mk调用Partitioner计算对应的Reduce task id。同一个task存在同一个文件上，放在本地硬盘上，文件按照mk自小到大排序。在这一步可以使用Combiner：Combiner（mk，{mv}）-&gt; &lt;mk,mv’&gt;，减少后续传输。</li><li>Shuffle：Reducer从每个Map Task传输中间结果文件，进行归并实现group by</li><li>Reduce</li></ol><p>图计算模型</p><ol><li>运算分成多个超步</li><li>超步内并行</li><li>超步间全局同步</li><li>相邻超步之间存在依赖</li></ol><p>基于顶点的编程模型</p><ol><li>每个顶点有一个value</li><li>顶点为中心的运算Compute函数可以接收消息，计算，发送消息等</li><li>顶点由活跃态（只对活跃态调用compute）和非活跃态（收到信息可以重新活跃）</li><li>所有顶点都是非活跃态，结束图运算</li></ol><p>Aggregator 全局统计量：每个超步内每个Worker分别进行本地的统计accumulate();超步间Master进行汇总；下一个超步Worker从Master得到上个超步的全局统计</p><hr><h2 id="GraphLab"><a href="#GraphLab" class="headerlink" title="GraphLab"></a>GraphLab</h2><ul><li>单机系统</li><li>共享内存（可以立即看到完成的计算结果）</li><li>异步计算</li></ul><h3 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h3><ul><li>Data graph G=(V,E)。每个顶点和边都可以有数据。</li><li>全局数据表(SDT)，定义全局可见的数据。</li></ul><h3 id="顶点计算"><a href="#顶点计算" class="headerlink" title="顶点计算"></a>顶点计算</h3><p>$Scope_v$是顶点计算涉及的范围，运算直接访问内存，update直接修改顶点和边上的数据，修改立即可见。</p><h3 id="共享内存"><a href="#共享内存" class="headerlink" title="共享内存"></a>共享内存</h3><p>解决数据竞争的方法：一致性模型</p><ul><li>Full consistency：不允许其他函数访问该Scope。</li><li>Edge consistency：update不写邻居顶点数据，但可能读，不允许其他函数会访问v及其邻边。</li><li>Vertex consistency：只读本顶点和v邻边数据，不允许其他函数访问v本身。</li></ul><p><img src="pictures/consistency.jpg" alt></p><hr><h2 id="PowerGraph"><a href="#PowerGraph" class="headerlink" title="PowerGraph"></a>PowerGraph</h2><p>问题：图划分有大量的跨边</p><p>解决：把大度顶点分裂成多个，看起来有多个主节点的副本</p><hr><h2 id="数据流系统Storm"><a href="#数据流系统Storm" class="headerlink" title="数据流系统Storm"></a>数据流系统Storm</h2><ul><li>计算形成一个有向无环图DAG（Topology数据结构表示，每个job有一个Topology）</li><li>Topology每一个顶点代表一个运算</li><li>边代表数据流动的关系</li></ul><p>上游节点输出分发到下游：</p><ul><li>随机分发</li><li>根据tuple指定域取值进行分发</li></ul><hr><h2 id="Hive-MapReduce-SQL"><a href="#Hive-MapReduce-SQL" class="headerlink" title="Hive(MapReduce + SQL)"></a>Hive(MapReduce + SQL)</h2><ul><li>数据存在HDFS</li><li>Table是一个单独的hdfs目录</li><li>Table进一步划分为Partition</li><li>Partition可以进一步划分为Bucket</li></ul><p>HDFS不支持修改：Insert是append操作；overwrite是删除然后新创建操作</p><hr><h2 id="内存处理"><a href="#内存处理" class="headerlink" title="内存处理"></a>内存处理</h2><p>去除I/O开销，提高处理速度</p><p>挑战：内存墙（解决：减少cache miss；预取指令降低cache miss对性能的影响）</p><p>MonetDB系统</p><h2 id="内存键值系统"><a href="#内存键值系统" class="headerlink" title="内存键值系统"></a>内存键值系统</h2><h3 id="Memcached"><a href="#Memcached" class="headerlink" title="Memcached"></a>Memcached</h3><p>单机内存键值对系统，hashtable形式</p><h3 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h3><p>分布式内存键值对，hash，list，sets，sorted sets等</p><hr><h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><p>解决MapReduce中多个运算采用同一个数据时，代价太高的问题</p><p>思路：把数据放入多台机器的内存，避免HDFS开销</p><h3 id="基础数据结构RDD"><a href="#基础数据结构RDD" class="headerlink" title="基础数据结构RDD"></a>基础数据结构RDD</h3><p>Transformation（RDD-&gt;RDD）<br>Action（RDD-&gt;计算结果）</p><p>读入内存一次可以多次处理</p><h3 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h3><p>DataFrame：可以看作RDD定义Relational Schema</p><h3 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h3><p>输入的数据流转化成一个个minibatch，在mininbatch上运算</p><p><img src="pictures/summary.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;MapReduce&quot;&gt;&lt;a href=&quot;#MapReduce&quot; class=&quot;headerlink&quot; title=&quot;MapReduce&quot;&gt;&lt;/a&gt;MapReduce&lt;/h2&gt;&lt;p&gt;程序员写串行程序，系统并行分布式执行。&lt;/p&gt;
&lt;p&gt;数据模型&amp;lt;key,val
      
    
    </summary>
    
      <category term="Database" scheme="http://www.liufr.com/categories/Database/"/>
    
    
      <category term="Database" scheme="http://www.liufr.com/tags/Database/"/>
    
  </entry>
  
  <entry>
    <title>大数据存储系统</title>
    <link href="http://www.liufr.com/2019/06/04/%E6%95%B0%E6%8D%AE%E5%BA%93/2019-06-04-%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F/"/>
    <id>http://www.liufr.com/2019/06/04/数据库/2019-06-04-大数据存储系统/</id>
    <published>2019-06-03T16:00:00.000Z</published>
    <updated>2019-06-05T02:38:05.122Z</updated>
    
    <content type="html"><![CDATA[<h2 id="NFS"><a href="#NFS" class="headerlink" title="NFS"></a>NFS</h2><p>POSIX文件系统</p><p>主要目的：</p><ul><li>从不同终端访问同一个目录</li><li>多用户共享数据</li><li>集中管理</li><li>不是处理大规模数据</li></ul><h3 id="设计目标1：服务器出现故障可以简单快速恢复。"><a href="#设计目标1：服务器出现故障可以简单快速恢复。" class="headerlink" title="设计目标1：服务器出现故障可以简单快速恢复。"></a>设计目标1：服务器出现故障可以简单快速恢复。</h3><p>Staeless(无状态)：NFS Server不保存任何状态，每个操作都是无状态的。<br>Idempoten(幂等性)：重复多次结果不变。（Read不改变数据；Write在相同位置写相同的数据）</p><p>Server Crash Recovery：Server只用重启；Client不断重试。</p><h3 id="设计目标2：远程文件操作性能高。"><a href="#设计目标2：远程文件操作性能高。" class="headerlink" title="设计目标2：远程文件操作性能高。"></a>设计目标2：远程文件操作性能高。</h3><p>在Client cache中缓存读写的数据。</p><p>可能会造成访问冲突问题，Cache不能保证一致性。<strong>解决方法</strong>:</p><ol><li>在文件关闭时必须把缓存的已修改的文件数据写回NFS Server。</li><li>每次使用缓存数据前必须检查是否已经过时（轮询操作影响性能）。</li></ol><h3 id="AFS"><a href="#AFS" class="headerlink" title="AFS"></a>AFS</h3><p>Invalidation</p><ul><li>Client 获得一个文件在server上登记</li><li>Server 发现文件被修改向登记的Client发送一个callback</li><li>Client 收到callback删除缓存的文件</li></ul><hr><h2 id="GFS-HDFS"><a href="#GFS-HDFS" class="headerlink" title="GFS/HDFS"></a>GFS/HDFS</h2><p>应用层文件系统（Meta data和Data分离）</p><ul><li>Name Node（存元数据）：文件名，长度，分成多少数据块，每个数据块分布在哪些Data Node上</li><li>Data Node：存数据块</li></ul><h3 id="Read"><a href="#Read" class="headerlink" title="Read"></a>Read</h3><ol><li>打开文件时与Name node通信一次</li><li>之后的读操作直接与Data Node通信</li><li>支持并发read</li></ol><h3 id="Write"><a href="#Write" class="headerlink" title="Write"></a>Write</h3><ol><li>写与Name node通信一次，返回应写的Data nodes（由Name Node决定）</li><li>CLient 发送数据给Primary和secondary datanode，形成流水线，实现备份，此时写入cache中还没写进HDFS</li><li>收到写命令才把缓存写入文件系统</li></ol><p>并发写的问题可能因为对于同一个数据块的两次操作，可能对之前的有覆盖（不支持此操作）；支持并行append。</p><hr><h2 id="NoSQL"><a href="#NoSQL" class="headerlink" title="NoSQL"></a>NoSQL</h2><h3 id="Dynamo"><a href="#Dynamo" class="headerlink" title="Dynamo"></a>Dynamo</h3><p>最简单的&lt;key,value&gt;模型，get/put操作</p><ul><li>单节点上存储由外部存储系统实现</li><li>多节点间的数据分布<ul><li>Consistent hashing</li><li>Quorum (N, W, R)</li><li>Eventual consistency</li></ul></li></ul><h4 id="一致性哈希"><a href="#一致性哈希" class="headerlink" title="一致性哈希"></a>一致性哈希</h4><p><img src="pictures/consistent_hash.jpg" alt><br><img src="pictures/consistent_hash1.jpg" alt></p><h4 id="3副本备份"><a href="#3副本备份" class="headerlink" title="3副本备份"></a>3副本备份</h4><p>Put到Node j上的数据，要备份到Node j+1和Node j+2上，所以一个Node j上实际存储的数据是$(u_{j-3}, u_j]$，增加或者删除节点时需要更改备份位置。</p><h4 id="Quorum机制：实现读写的一致性"><a href="#Quorum机制：实现读写的一致性" class="headerlink" title="Quorum机制：实现读写的一致性"></a>Quorum机制：实现读写的一致性</h4><p>Quorum（N，W，R）</p><ul><li>由N个副本</li><li>写:保证&gt;=W个副本的写完成</li><li>读:读&gt;=R个副本，选出最新版本</li><li>如果R+W&gt;N，那么一定读到最新的数据</li><li>R小，读效率高</li><li>W小，写效率高</li></ul><p>Put操作</p><ul><li>Client根据hash(key)得到所有N个副本所在的节点</li><li>Client向所有N个副本所在的节点发出put</li><li>等到至少W个节点完成的响应，就认为写成功</li></ul><p>Get操作</p><ul><li>Client根据hash(key)得到所有N个副本所在的节点</li><li>Client向所有N个副本所在的节点发出get</li><li>等到至少R个节点的value，就必然包含最新一次写的值</li></ul><hr><h3 id="Bigtable-HBase"><a href="#Bigtable-HBase" class="headerlink" title="Bigtable/HBase"></a>Bigtable/HBase</h3><p><img src="pictures/bigtable.jpg" alt></p><ul><li><p>Key包括row key与column两个部分</p></li><li><p>所有row key是按顺序存储的</p></li><li><p>其中column又有column family前缀</p><ul><li>Column family是需要事先声明的，种类有限（例如<del>10或</del>100）</li><li>而column key可以有很多</li></ul></li><li><p>具体存储时，每个column family将分开存储（类似列式数据库）</p></li><li><p>Get</p><ul><li>给定row key, column family, column key</li><li>读取value</li></ul></li><li><p>Put</p><ul><li>给定row key, column family, column key</li><li>创建或更新value</li></ul></li><li><p>Scan</p><ul><li>给定一个范围，读取这个范围内所有row key的value</li><li>Row key是排序存储的</li></ul></li><li><p>Delete</p><ul><li>删除一个指定的value</li></ul></li></ul><p>Tablet是分布式Bigtable表的一部分</p><p>查找Tablet：三层的B+-Tree，每个叶子节点是一个Tablet，内部节点是特殊的MetaData Tablet，其包含Tablet的位置信息。</p><ul><li>Put<ul><li>写入MemTable，Append日志</li><li>MemTable满了就对其排序，生成新的SSTable，只创建一次不修改最后删除。</li></ul></li><li>Get<ul><li>对每个SSTable都建索引</li></ul></li></ul><hr><h3 id="Cassandra-Dynamo-Bigtable"><a href="#Cassandra-Dynamo-Bigtable" class="headerlink" title="Cassandra(Dynamo+Bigtable)"></a>Cassandra(Dynamo+Bigtable)</h3><p><img src="pictures/Cassandra.jpg" alt></p><hr><h3 id="分布式协调（ZooKeeper）"><a href="#分布式协调（ZooKeeper）" class="headerlink" title="分布式协调（ZooKeeper）"></a>分布式协调（ZooKeeper）</h3><p>多个ZooKeeper维护一组共同的数据状态，2f+1个ZooKeeper可以容忍f个节点故障。</p><p>ZooKeeper是一个简化的文件系统，树结构，可以用一条从根开始的路径确定。</p><p>Client Session：开始（Client主动连接），结束（CLient主动关闭，或者经过一个Timeout，Zookeeper没有收到通信）</p><p>Watch机制：exists可以判断节点是否存在；ZooKeeper通知，通知后Watch被删除。</p><p>同步机制：请求后阻塞；异步机制：允许Client发多个请求，提供回调函数。</p><p>Zookeeper需要保证写操作串行化（由leader领导follower按照相同顺序完成，FIFO，无保证，要调用sync）</p><p>ZAB（一致性协议）</p><ul><li>Propose阶段<ul><li>Leader把一个新的txn写入本地log， 广播Propose这个txn </li><li>每个Follower收到Propose后，写入本地log，向Leader发回Ack</li></ul></li><li>Commit阶段<ul><li>Leader收到 f个Ack后，写Commit到log, 广播Commit，然后修改自己的ZooKeeper树</li><li>Follower收到Commit消息，写Commit到log，然后修改ZooKeeper树、</li></ul></li><li>Recovery<ul><li>竞选leader（Txn ID最大的节点）</li><li>新的leader包正确执行的Txn都正确执行，丢弃未执行的操作</li></ul></li></ul><hr><h2 id="JSON"><a href="#JSON" class="headerlink" title="JSON"></a>JSON</h2><h3 id="JSON-vs-Google-Protocol-Buffers"><a href="#JSON-vs-Google-Protocol-Buffers" class="headerlink" title="JSON vs Google Protocol Buffers"></a>JSON vs Google Protocol Buffers</h3><ul><li>相同点：<ul><li>都可以表达程序设计语言中的结构和数组</li><li>嵌套：JSON object, PB message/group</li><li>数组：JSON array, PB repeated</li><li>缺值的情况：JSON记录实际上没有规定一定要有什么域，PB optional</li></ul></li><li>不同点：<ul><li>数据类型：PB要求事先声明，JSON不需要</li></ul></li></ul><h3 id="JSON-vs-XML"><a href="#JSON-vs-XML" class="headerlink" title="JSON vs XML"></a>JSON vs XML</h3><ul><li>JSON；轻量，自定义格式，key-value</li><li>XML：重量，需要定义格式</li></ul><hr><h2 id="Document-Store"><a href="#Document-Store" class="headerlink" title="Document Store"></a>Document Store</h2><ul><li>Database &lt;-&gt; 数据库</li><li>Collection &lt;-&gt; Table概念</li><li>Document &lt;-&gt; 记录概念</li></ul><table><thead><tr><th align="right">MongoDB</th><th align="right">SQL</th></tr></thead><tbody><tr><td align="right">insert</td><td align="right">create</td></tr><tr><td align="right">insert</td><td align="right">insert</td></tr><tr><td align="right">find</td><td align="right">select</td></tr><tr><td align="right">aggregate/group</td><td align="right">group by</td></tr><tr><td align="right">不支持join</td><td align="right">join</td></tr><tr><td align="right">支持index</td><td align="right"></td></tr></tbody></table><p>Write concern：</p><ol><li>Unacknowledged: 写请求发送了，就认为完成</li><li>Acknowledged：MongoDB应答了收到写请求，就认为完成</li><li>Journaled：MongoDB把写请求记录在硬盘上的日志中，认为完成</li></ol><hr><h2 id="图存储系统"><a href="#图存储系统" class="headerlink" title="图存储系统"></a>图存储系统</h2><h3 id="Neo4j"><a href="#Neo4j" class="headerlink" title="Neo4j"></a>Neo4j</h3><ul><li>顶点 &lt;-&gt; node</li><li>边 &lt;-&gt; relationship, 双向链表</li><li>顶点和边存储多个key-value值 &lt;-&gt; property，单向链表</li></ul><p>对node relationship property由缓冲区</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;NFS&quot;&gt;&lt;a href=&quot;#NFS&quot; class=&quot;headerlink&quot; title=&quot;NFS&quot;&gt;&lt;/a&gt;NFS&lt;/h2&gt;&lt;p&gt;POSIX文件系统&lt;/p&gt;
&lt;p&gt;主要目的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;从不同终端访问同一个目录&lt;/li&gt;
&lt;li&gt;多用户共享数据&lt;
      
    
    </summary>
    
      <category term="Database" scheme="http://www.liufr.com/categories/Database/"/>
    
    
      <category term="Database" scheme="http://www.liufr.com/tags/Database/"/>
    
  </entry>
  
  <entry>
    <title>关系数据库</title>
    <link href="http://www.liufr.com/2019/06/03/%E6%95%B0%E6%8D%AE%E5%BA%93/2019-06-03-%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    <id>http://www.liufr.com/2019/06/03/数据库/2019-06-03-关系数据库/</id>
    <published>2019-06-02T16:00:00.000Z</published>
    <updated>2019-06-04T03:07:03.013Z</updated>
    
    <content type="html"><![CDATA[<p>大数据的概念(三个重要挑战)</p><ol><li>数据量巨大(Volumn)</li><li>数据的产生 速度、更新速度快(Velocity)</li><li>数据种类繁多(Variety)</li></ol><p>大数据管理系统：</p><ol><li>关系型（Oracle，DB2，MS SQL）</li><li>云平台（MapReduce，Apache Hadoop，MS Dryad）</li><li>云平台+SQL（Apache Hive）</li><li>No-SQL（Apache Hbase，MongoDB，Neo4j）</li><li>图数据处理（Google Pregel，Apache Giraph，Graphlab）</li><li>内存数据处理（MMDB，Spark）</li></ol><p>ACID：</p><ol><li>原子性（Atomicity）</li><li>一致性（Consistency）</li><li>隔离性（Isolation）</li><li>持久性（Durability）</li></ol><hr><h2 id="关系型数据库"><a href="#关系型数据库" class="headerlink" title="关系型数据库"></a>关系型数据库</h2><h3 id="Table-Relation-表"><a href="#Table-Relation-表" class="headerlink" title="Table/Relation(表)"></a>Table/Relation(表)</h3><ul><li>列：一个属性，有明确的数据类型</li><li>行：一个记录</li><li>通常很瘦长</li><li>原子类型（无内部嵌套结构，不能用struct、class、array、list、set、map等）</li></ul><h3 id="Scheme-vs-Instance"><a href="#Scheme-vs-Instance" class="headerlink" title="Scheme vs Instance"></a>Scheme vs Instance</h3><ul><li>Scheme：一个表的类型是由每个列的类型决定的</li><li>Instance：具体取值，由具体应用决定</li><li>Scheme定义一次对应多个instance</li></ul><h3 id="Key"><a href="#Key" class="headerlink" title="Key"></a>Key</h3><ul><li>Primary key主键确定本表中的一个记录(可以是多个键的组合)；</li><li>Foreign key外键唯一决定另一个表的记录；</li></ul><hr><h2 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h2><figure class="highlight sql hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> 表名 (</span><br><span class="line">列名 类型,</span><br><span class="line">列名 类型,</span><br><span class="line">列名 类型,</span><br><span class="line">…… ,</span><br><span class="line">primary <span class="hljs-keyword">key</span>(<span class="hljs-keyword">ID</span>) -&gt; 声明主键，可以包含多个属性</span><br><span class="line">foreign <span class="hljs-keyword">key</span>(CourseId) <span class="hljs-keyword">references</span> Course(<span class="hljs-keyword">ID</span>),</span><br><span class="line">foreign <span class="hljs-keyword">key</span>(StudentId) <span class="hljs-keyword">references</span> Student(<span class="hljs-keyword">ID</span>) -&gt; 声明外键，可以有多个</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">插入完整记录：</span><br><span class="line"><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> Student</span><br><span class="line"><span class="hljs-keyword">values</span> (<span class="hljs-number">131234</span>, ‘张飞’, <span class="hljs-number">1995</span>/<span class="hljs-number">1</span>/<span class="hljs-number">1</span>, M, ‘计算机’, <span class="hljs-number">2013</span>, <span class="hljs-number">85</span>);</span><br><span class="line"></span><br><span class="line">插入记录特定的列，其它列为空：</span><br><span class="line"><span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> Student(<span class="hljs-keyword">ID</span>, <span class="hljs-keyword">Name</span>)</span><br><span class="line"><span class="hljs-keyword">values</span> (<span class="hljs-number">131234</span>, ‘张飞’);</span><br><span class="line"></span><br><span class="line">删除记录</span><br><span class="line"><span class="hljs-keyword">delete</span> <span class="hljs-keyword">from</span> Student</span><br><span class="line"><span class="hljs-keyword">where</span> <span class="hljs-keyword">ID</span> = <span class="hljs-number">131234</span>;</span><br><span class="line"></span><br><span class="line">更新</span><br><span class="line"><span class="hljs-keyword">update</span> Student</span><br><span class="line"><span class="hljs-keyword">set</span> GPA = <span class="hljs-number">86</span></span><br><span class="line"><span class="hljs-keyword">where</span> <span class="hljs-keyword">ID</span> = <span class="hljs-number">131234</span></span><br><span class="line"></span><br><span class="line">选择（从一个表中提取一些行）</span><br><span class="line"><span class="hljs-keyword">select</span> *</span><br><span class="line"><span class="hljs-keyword">from</span> 表名</span><br><span class="line"><span class="hljs-keyword">where</span> 条件（多个条件可以用<span class="hljs-keyword">and</span>,<span class="hljs-keyword">or</span>,()进行组合）</span><br><span class="line"></span><br><span class="line">投影（从一个表中提取一些列）</span><br><span class="line"><span class="hljs-keyword">select</span> 列名，...列名</span><br><span class="line"><span class="hljs-keyword">from</span> 表名</span><br><span class="line"></span><br><span class="line">选择+投影</span><br><span class="line"><span class="hljs-keyword">select</span> <span class="hljs-keyword">Name</span>,GPA</span><br><span class="line"><span class="hljs-keyword">from</span> Student</span><br><span class="line"><span class="hljs-keyword">where</span> Major=<span class="hljs-string">"计算机"</span></span><br><span class="line"></span><br><span class="line">连接(<span class="hljs-keyword">Join</span>)</span><br><span class="line"><span class="hljs-keyword">select</span> Student.Name, Course.Name</span><br><span class="line"><span class="hljs-keyword">from</span> Student, Course, TakeCourse</span><br><span class="line"><span class="hljs-keyword">where</span> TakeCourse.CourseId = Course.ID <span class="hljs-keyword">and</span> TakeCourse.StudentID = Student.ID;</span><br><span class="line"></span><br><span class="line">分组统计(group by)</span><br><span class="line"><span class="hljs-keyword">select</span> Major, <span class="hljs-keyword">count</span>(*)</span><br><span class="line"><span class="hljs-keyword">from</span> Student</span><br><span class="line"><span class="hljs-keyword">where</span> <span class="hljs-keyword">Year</span> &gt;= <span class="hljs-number">2013</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">Year</span> &lt;= <span class="hljs-number">2014</span></span><br><span class="line"><span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> Major;</span><br><span class="line"></span><br><span class="line">过滤(Having,在group by基础上选择)</span><br><span class="line"><span class="hljs-keyword">select</span> Major, <span class="hljs-keyword">count</span>(*) <span class="hljs-keyword">as</span> Cnt</span><br><span class="line"><span class="hljs-keyword">from</span> Student</span><br><span class="line"><span class="hljs-keyword">where</span> <span class="hljs-keyword">Year</span> &gt;= <span class="hljs-number">2013</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">Year</span> &lt;= <span class="hljs-number">2014</span></span><br><span class="line"><span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> Major</span><br><span class="line"><span class="hljs-keyword">having</span> Cnt &gt;= <span class="hljs-number">2</span>;</span><br><span class="line"></span><br><span class="line">排序(Order by,desc减少，asc增加)</span><br><span class="line"><span class="hljs-keyword">select</span> Major, <span class="hljs-keyword">count</span>(*) <span class="hljs-keyword">as</span> Cnt</span><br><span class="line"><span class="hljs-keyword">from</span> Student</span><br><span class="line"><span class="hljs-keyword">where</span> <span class="hljs-keyword">Year</span> &gt;= <span class="hljs-number">2013</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">Year</span> &lt;= <span class="hljs-number">2014</span></span><br><span class="line"><span class="hljs-keyword">group</span> <span class="hljs-keyword">by</span> Major</span><br><span class="line"><span class="hljs-keyword">order</span> <span class="hljs-keyword">by</span> Cnt <span class="hljs-keyword">desc</span>;</span><br></pre></td></tr></table></figure><hr><h3 id="RDBMS-关系型数据库系统"><a href="#RDBMS-关系型数据库系统" class="headerlink" title="RDBMS(关系型数据库系统)"></a>RDBMS(关系型数据库系统)</h3><p>系统架构</p><ul><li>SQL Parse:SQL语句的程序（语法解析，表名，列名，类型检查）</li><li>Query Optimizer：SQL内部表达（产生可行的query plan选最佳）</li><li>Data storage and indexing（如何在硬盘上存储访问数据）</li><li>Buffer pool：在内存中缓存硬盘数据</li><li>Execution Engine：（根据query plan完成相应的运算和操作）</li><li>Transaction management：事务管理（ACID，logging日志加锁，保证并行事务正确性）</li></ul><hr><h3 id="数据库-vs-文件系统"><a href="#数据库-vs-文件系统" class="headerlink" title="数据库 vs 文件系统"></a>数据库 vs 文件系统</h3><table><thead><tr><th align="right">文件系统</th><th align="right">数据库</th></tr></thead><tbody><tr><td align="right">存储文件</td><td align="right">存储数据表</td></tr><tr><td align="right">通用的</td><td align="right">专用的</td></tr><tr><td align="right">文件无结构，由一串字节组成</td><td align="right">由记录组成，每个记录有多个属性</td></tr><tr><td align="right">操作系统实现</td><td align="right">用户态程序实现</td></tr><tr><td align="right">提供编程接口</td><td align="right">提供SQL接口</td></tr></tbody></table><p>相同点：数据存储在外存；数据分成定长数据块</p><p>RDBMS最小存储单位database page size</p><p>Tuple可以存储变长的列,主要就是利用指针把边长空间放到后面</p><p><img src="pictures/tunple.jpg" alt><br><img src="pictures/tunple_example.jpg" alt></p><p>数据的顺序访问会造成性能问题，所以采用有选择的访问</p><p>索引</p><ul><li>Tree based index：有序，支持点查询和范围查询（每个叶子节点是一个page，内部节点索引用）</li><li>Hash based index：无序，只支持点查询（每个bucket是一个page，$O(log_2^N)$）</li><li>主索引：记录存在index中，顺序为index顺序</li><li>二级索引：index存page ID和in-page tuple slot ID</li></ul><p>数据访问有空间和时间局部性</p><p>buffer pool如果在替换时没有空闲frame，找到已缓存的page（LRU）被修改过需要写回硬盘</p><p>LRU的实现：</p><ol><li>时间戳最小的页$O(n)$</li><li>循环链表，当一个页被访问放到链表最前端，然后替换最后一个页$O(1)$(修改队列代价大且多线程需要共享队头)</li><li>Clock算法：访问R=1；旋转R==1-&gt;R=0;R==0选中。（说明在旋转一圈时间里都没有被访问）</li></ol><hr><h3 id="Operator-Tree"><a href="#Operator-Tree" class="headerlink" title="Operator Tree"></a>Operator Tree</h3><p>每个节点代表一个运算；输入来自孩子节点；输出送往父节点。<br><img src="pictures/operate_tree.jpg" alt></p><ul><li>Selection:行过滤（比较操作、数学运算、逻辑运算）</li><li>Projection:列提取（从记录中提取属性生成结果记录）</li></ul><hr><h3 id="Join"><a href="#Join" class="headerlink" title="Join"></a>Join</h3><ul><li>Nested loop（双重循环，总共读$M_R+BM_RM_S$）</li><li>Block Nested Loop Join(在外循环每次读入M页的R而不是一条R记录，减少了内循环总共读$M_R+M_RM_S/M$)</li><li>Index Nested Loop Join(设置一个中间index用来检索S表，当很少有匹配时效率较高)</li><li>Hashing（读R建立Hash table，读S访问hash table找到所有的匹配；当R大于内存可以分为小块，由于匹配的记录hash值相同因此必然存在与对应的块中，块id=hash(join key)%分块数量。总共读$2M_R+2M_S$个page，写$M_R+M_S$个）</li><li>Sort Merge Join（先排序，后merge）</li><li>Sorting</li></ul><hr><p>事务的正确性：存在一个顺序，按照这个顺序依次串行执行这些Transactions，得到的结果与并行执行相同。</p><p>数据冲突引起的问题：</p><ul><li>读脏数据（写读）</li><li>不可重复读（读写）</li><li>更新丢失（写写）</li></ul><p>解决方案：</p><ul><li>悲观：假设数据竞争可能经常出现；防止竞争的出现（等待）</li><li>乐观：假设数据竞争很少见；先执行然后提交前检查是否没有数据竞争</li></ul><p>加锁：<br>对每个访问的数据都要加锁后才能访问，commite前集中解锁。</p><ul><li>共享锁(S)：保护读操作</li><li>互斥锁(X)：保护写操作</li><li>IS(a):将对a下面更细粒度的数据元素进行读</li><li>IX(a):将对a下面更细粒度的数据进行写</li></ul><p>下面这个图分四个2*2观察，只有左上角全绿，其余三个时一样的。<br><img src="pictures/lock.jpg" alt></p><p>循环等待时的deadlock(死锁)</p><ul><li>死锁避免：规定lock对象顺序，按照顺序请求lock，适用于lock少的情况。由于数据库lock多不适合该方法。</li><li>死锁检测：对长期等待的事务检查，如果有死锁则对环上任意一个事务丢弃处理。</li></ul><p>乐观的并发控制：读；验证；写。（冲突多时可能要不断重试浪费资源）<br>Snapshot Isolation：在起始点snapshot数据先临时保存commit检查冲突。</p><p>事务日志：记录一个写操作的全部信息</p><ol><li>写操作：产生一个事务日志记录</li><li>Commit：产生一个commit日志记录</li><li>Abort：产生一个abort日志记录</li></ol><p>Write-Ahead logging：先logging后实际操作。先记录commit日志然后在commit。</p><p>WAL的持久性：出现掉电可以回溯日志，寻找commit日志。必须保证日志记录先于修改后的数据出现在硬盘上。</p><p>checkpoint：使崩溃恢复时间可控。</p><p>崩溃恢复：</p><ol><li>检查</li><li>找到最后一个检查点</li><li>找到日志崩溃点</li><li>确定崩溃时的活跃事务和脏页</li><li>REDO阶段</li><li>找到脏页最早的LSN</li><li>从LSN正向读取日志</li><li>如果日志涉及的页不在脏页中跳过，数据页LSN&gt;=日志的LSN跳过；否则修改数据页。</li><li>UNDO阶段</li><li>对于所有在崩溃时活跃的事务找到最新LSN，通过反向链表读取其所有日志记录。</li><li>如果数据页LSN&gt;=日志LSN，才进行undo</li></ol><hr><table><thead><tr><th align="right">数据仓库</th><th align="right">事务处理</th></tr></thead><tbody><tr><td align="right">少数数据分析操作</td><td align="right">大量并发transactions</td></tr><tr><td align="right">每个操作访问大量数据</td><td align="right">访问很少数据</td></tr><tr><td align="right">读</td><td align="right">读写</td></tr></tbody></table><hr><h3 id="OLAP-联机分析处理"><a href="#OLAP-联机分析处理" class="headerlink" title="OLAP(联机分析处理)"></a>OLAP(联机分析处理)</h3><p>数据仓库的基础。基本数据模型：多维矩阵。称为数据立方。</p><p>常用操作：</p><ul><li>rollup（上卷-&gt;在某维度从细粒度到粗粒度），</li><li>drill down（下钻-&gt;从粗粒度到细粒度）</li><li>slice（切片-&gt;在某一维上选一个值）</li><li>dice（切块-&gt;在多维上选多个值）</li></ul><p>行式数据存储：每个记录中所有的列都相邻存放。多个列通过一个I/O得到<br>列式数据存储：每个列产生一个文件，（因为大部分情况只涉及一个表的少数几列，数据更容易压缩，拼装代价大）</p><hr><h3 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h3><p>划分：把数据分布在多台服务器上machine Id=hash(key) % MachineNumber。对与join操作，如果划分key等于join key可以直接进行；如果不是需要在join key进行一次划分然后再join。</p><p>2-phase commit</p><ol><li>Coordinator 向每个participant 发送query to commit消息，participant根据本地情况回答yes/no。</li><li>当所有回答为yes进行commit，participant发送acknowledgment</li><li>至少一个回答no进行abort</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;大数据的概念(三个重要挑战)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;数据量巨大(Volumn)&lt;/li&gt;
&lt;li&gt;数据的产生 速度、更新速度快(Velocity)&lt;/li&gt;
&lt;li&gt;数据种类繁多(Variety)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;大数据管理系统：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
      
    
    </summary>
    
      <category term="Database" scheme="http://www.liufr.com/categories/Database/"/>
    
    
      <category term="Database" scheme="http://www.liufr.com/tags/Database/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://www.liufr.com/2019/05/30/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/~$19-05-28-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0(2)/"/>
    <id>http://www.liufr.com/2019/05/30/强化学习/~$19-05-28-强化学习(2)/</id>
    <published>2019-05-30T06:55:19.080Z</published>
    <updated>2019-05-30T06:55:19.080Z</updated>
    
    <content type="html"><![CDATA[<p>liufrliufr�数只在事件结束时离线更新, 那么所有的更新量等������r��P ���</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;liufr                                                 l i u f r   �数只在事件结束时离线更新, 那么所有的更新量等    ���    ��� r��P ��  �&lt;/p&gt;

      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>强化学习(3)</title>
    <link href="http://www.liufr.com/2019/05/29/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/2019-05-29-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0(3)/"/>
    <id>http://www.liufr.com/2019/05/29/强化学习/2019-05-29-强化学习(3)/</id>
    <published>2019-05-28T16:00:00.000Z</published>
    <updated>2019-05-29T09:43:28.842Z</updated>
    
    <content type="html"><![CDATA[<p><strong>在策略学习</strong>根据策略$\pi$ 产生的样本来学习关于$\pi$的相关知识<br><strong>离策略学习</strong>根据另一个策略$\mu$产生的样本来学习关于$\pi$的相关知识(智能体观察人；重复利用旧策略；探索性策略学习最优策略；单一策略去学习多个策略)</p><p>$\epsilon-贪心探索$:以$1−\epsilon$概率选择贪心动作 以$\epsilon$概率随机选择一个动作 </p><p>$\epsilon-贪心策略提升$给定任意$\epsilon$-贪心策略$\pi$, 根据 $q_{\pi}$ 构造出新的 $\epsilon$-贪心策略 $\pi′$ 具有 更好的性能, 即 $v_{\pi′}(s) ≥v_{\pi}(s)$</p><p>无限探索, 无穷时刻收敛为贪心策略（GLIE）的含义是:智能体能够无限次数地探索所有的状态-动作对;策略在无穷时刻收敛到贪心策略。</p><p>GLIE蒙特卡洛控制：能够收敛到最优动作 -价值函数, $Q(s,a) \rightarrow q∗(s,a)$</p><ul><li>使用策略$\pi$采集第$k$次事件: ${S_1,A_1,R_2,…,S_T}∼ \pi$ </li><li>对事件中的每个状态 $S_t$ 和动作 $A_t$:$N(S_t,A_t)\leftarrow N(S_t,A_t)+1;Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\frac{1}{N(S_t,A_t)}(G_t-Q(S_t,A_t))$ </li><li>基于新得到的动作-价值函数对策略进行提升$\epsilon \leftarrow \frac{1}{k};\pi \leftarrow\epsilon-greedy(Q)$</li></ul><hr><h3 id="Sarsa-S-A-R-S’-A’"><a href="#Sarsa-S-A-R-S’-A’" class="headerlink" title="Sarsa(S,A,R,S’,A’)"></a>Sarsa(S,A,R,S’,A’)</h3><p>基于样本的TD更新<br>$Q(S,A) \leftarrow Q(S,A) + \alpha(R+ \gamma Q(S^′,A^′)−Q(S,A)$</p><ol><li>任意初始化 $Q(s,a)$, 令 $Q(S_{terminal},\cdot) = 0$ </li><li>repeat {在每次事件中:} </li><li>初始化 S </li><li>根据从 Q 提取的策略 (例如 $\epsilon$-贪心策略) 对 S 选择动作 A </li><li>repeat {对事件中的每一时刻} </li><li>执行动作 A, 观察 $R,S′$ </li><li>根据从 Q 提取的策略 (例如 $\epsilon$-贪心策略) 对 S′ 选择动作 A′ </li><li>$Q(S,A) \leftarrow Q(S,A) + \alpha(R+ \gamma Q(S^′,A^′)−Q(S,A)$</li><li>$S\leftarrow S^′, A\leftarrow A^′$</li><li>until S 是终止状态 </li><li>until</li></ol><p>Sarsa vs MC</p><ul><li>MC可能无法达到终止状态（例如学习到呆在原地不动）</li><li>Sarsa每一步都在学习，转向其他策略</li></ul><p>一步 Sarsa vs Sarsa($\lambda$)</p><ul><li>一步 Sarsa 只对最终导致高奖励的<strong>最后一步动作</strong>强化它的价值 </li><li>资格迹方法能够对事件中的<strong>多个动作强化</strong>它们的价值,步数增加强化幅度减小；衰减率$\gamma\lambda$</li></ul><hr><h3 id="Q-学习"><a href="#Q-学习" class="headerlink" title="Q-学习"></a>Q-学习</h3><ul><li>考虑基于动作-价值 Q(s,a) 的离策略学习 </li><li>不再使用重要性采样 </li><li>智能体下一时刻执行的动作是由行为策略产生$A_{t+1} ∼ \mu(\cdot|S_t)$</li><li>但是学习算法考虑的是由另一个目标策略产生的后继动作 $A^′ ∼ \pi(\cdot|S_t)$ </li><li>更新 $Q(S_t,A_t)$ 向另一个后继动作的价值逼近 $Q(St,At) \leftarrow Q(S_t,A_t)+\alpha(R_{t+1} + \gamma Q(S_{t+1},A^′)−Q(S_t,A_t))$</li></ul><ol><li>任意初始化 Q(s,a), 令 $Q(Sterminal,\cdot) = 0$ </li><li>repeat {在每次事件中:} </li><li>初始化 S </li><li>repeat {对事件中的每一时刻} </li><li>根据从 Q 提取的策略 (例如 $\epsilon$-贪心策略) 对 S 选择动作 A </li><li>执行动作 A, 观察 R,S′ </li><li>$Q(St,At) \leftarrow Q(S_t,A_t)+\alpha(R_{t+1} + \gamma Q(S_{t+1},A^′)−Q(S_t,A_t))$</li><li>$S\leftarrow S^′$ </li><li>until S 是终止状态 </li><li>until</li></ol><p>对于悬崖问题：</p><ul><li>Q-学习：最优路径；Sarsa安全路径（考虑到了随机探索）</li><li>即使 Sarsa 学到的安全路径比 Q-学习的最优路径行走步数要长, 但是每次获得的奖励和却比 Q-学习的高</li></ul><hr><p><strong>利用</strong>: 根据当前的信息做出最佳的决策； <strong>探索</strong>: 采样更多的信息 想要做出长期的最佳决</p><p>对于Q学习来说，$\epsilon=0$学习结果容易陷入局部最优；$\epsilon$过大探索整个空间，降低回报。探索率随时间衰减</p><p>$\epsilon$贪心根据动作-价值决定动作被选中的概率</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;在策略学习&lt;/strong&gt;根据策略$\pi$ 产生的样本来学习关于$\pi$的相关知识&lt;br&gt;&lt;strong&gt;离策略学习&lt;/strong&gt;根据另一个策略$\mu$产生的样本来学习关于$\pi$的相关知识(智能体观察人；重复利用旧策略；探索性策略学习最优策略
      
    
    </summary>
    
      <category term="RL" scheme="http://www.liufr.com/categories/RL/"/>
    
    
      <category term="RL" scheme="http://www.liufr.com/tags/RL/"/>
    
  </entry>
  
  <entry>
    <title>强化学习(2)</title>
    <link href="http://www.liufr.com/2019/05/28/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/2019-05-28-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0(2)/"/>
    <id>http://www.liufr.com/2019/05/28/强化学习/2019-05-28-强化学习(2)/</id>
    <published>2019-05-27T16:00:00.000Z</published>
    <updated>2019-05-29T03:20:17.186Z</updated>
    
    <content type="html"><![CDATA[<h3 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h3><p>通过把原问题分解为相对简单的子问题来求解复杂问题 </p><p>马尔可夫决策过程满足如下两个属性 </p><ul><li>贝尔曼方程具有递归形式 </li><li>价值函数可以保存和重复利用</li></ul><p>贝尔曼最优方程如下,其难点是$v_<em>$同时存在于等式左右两边<br>$$<br>v_</em>(s)=max_a(R_s^a+\gamma \sum_{s’\in S}P_{ss’}^av_*(s’))<br>$$</p><p>价值迭代的基本思路: </p><ol><li>对$v_∗$定义一个估计函数$v$ </li><li>将估计函数代入方程右边, 等式左边得到一个新函数$v′$ </li><li>$v′$ 是对 $v_∗$ 更为准确的估计</li><li>将 $v′$ 代入右式继续上述过程</li></ol><p>$$<br>v’(s)=max_a(R_s^a+\gamma \sum_{s’\in S}P_{ss’}^av(s’))<br>$$</p><p><strong>价值迭代算子</strong>定义一个以函数作为输入的算子$\tau$, 对给定的函数$v_k$计算新的函数 $v_{k+1}(s)=<a href="s">\tau(v_k)</a>$,贝尔曼最优方程写成<br>$$<br>v_<em>(s)=[\tau(v_</em>)](s)=max_a(R_s^a+\gamma\sum_{s’\in S}P_{ss’}^av_*(s’))<br>$$<br>$\gamma &lt; 1$ 时, 价值迭代算子是一个收缩算子($||\tau(f-g)||_{\infty}&lt;||f-g||$)</p><p>确定性策略：价值迭代找到最优策略；随机性策略：机器人在所有位置以相同的概率向左或向右。</p><p>策略迭代：</p><ol><li>给定一个初始策略 π1, k = 1 </li><li>loop </li><li>策略评估: 对当前策略 πk 计算它的价值函数<br>$v_{πk}(s) = \mathbb E[R_{t+1} + \gamma R_{t+2} + …|<em>St = s,A_t ∼ π_k(St)] =\sum_a π_k(a|s)(R^a</em>{ss’} + \gamma P^a_{ss′}v_{πk}(s′))$</li><li>策略提升: 根据 $v_{πk}$ 提取出新的策略 $π<em>{k+1}(s) = argmax</em>{a\in A}(R^a_s + \gamma \sum_{s′\in S} P^a_{ss′}v_{πk}(s′))$</li><li>k←k+ 1 </li><li>end loop</li></ol><p>价值迭代 vs 策略迭代</p><p><strong>价值迭代</strong> </p><ol><li>只在收敛得到 $v∗$ 后计算 $π∗$, 中间过程不产生策略 </li><li>涉及赋值操作, 计算量小, $O(|S|^2|A|)$ </li><li>迭代次数多</li></ol><p><strong>策略迭代</strong></p><ol><li>每次迭代开始时给定一个 $π$, 结束时产生一个新 $π′$ </li><li>求解方程, 计算量大, 矩阵求逆 $O(|S|^3)$, 策略提升 $O(|S|^2|A|)$ </li><li>迭代次数少</li></ol><p><strong>异步动态规划</strong>对每个状态单独更新价值函数值, 可以以任意一种顺序选择被更新的状态 只对被选中的状态更新价值, 未选中的保持不变 </p><ol><li><strong>就地</strong>价值迭代只对整个状态集存储一个价值函数,</li><li><strong>优化动态规划</strong>对拥有最大贝尔曼误差的状态更新价值。</li><li><strong>实时动态规划</strong> 只考虑与智能体直接相关的状态</li></ol><p><strong>策略评估的不足</strong>：对模型的依赖$R,P$(MDP 问题的模型已知/智能体对环境建模)</p><hr><h3 id="蒙特卡洛方法MC"><a href="#蒙特卡洛方法MC" class="headerlink" title="蒙特卡洛方法MC"></a>蒙特卡洛方法MC</h3><ol><li>MC 方法直接从经历过的事件中学习 </li><li>无模型方法: 不需要 MDP 的转移/奖励函数 </li><li>从完整的事件中学习: 没有自举 </li><li>基于最简单的思想: 价值 = 平均回报 </li><li>运行 MC 方法通常要求: 所有事件都到达终止状态或者事件的时序足够长</li></ol><ul><li>目标: 从策略 $\pi$ 产生的事件中学习 $v_{\pi}$ $S_1,A_1,R_2,…,S_k ∼ \pi$ </li><li>回忆下回报的定义是所有折扣奖励和 $G_t = R_{t+1} + \gamma R_{t+2} +···+ \gamma^{T−1}R_t$</li><li>回忆下价值函数的定义是回报的期望 $v_π(s) = \mathbb E \pi[G_t|S_t = s]$ </li><li>蒙特卡洛策略评估方法使用回报的经验均值作为回报的期望</li></ul><p>每次经过的 MC 策略评估</p><ul><li>为了评估状态 s </li><li>对一次事件中每一次经过状态 s 的时刻 t </li><li>计数加一 $N(s) \leftarrow N(s) + 1$ </li><li>全部回报相加 $S(s) \leftarrow S(s) +G_t$ </li><li>估计的价值等于回报均值 $V(s) = S(s)/N(s)$</li><li>同样当 $N(s) \rightarrow \infty$ 时, $V(s) \rightarrow v_π(s)$</li></ul><p>P75 怎么算？？？？</p><p>MC可以使用增量计算$V(S_t)\leftarrow V(S_t)+\frac{1}{N(S_t)}(G_t-V(S_t))$</p><hr><h3 id="时间差分学习TD"><a href="#时间差分学习TD" class="headerlink" title="时间差分学习TD"></a>时间差分学习TD</h3><ol><li>TD 方法直接从智能体经历的事件中学习 </li><li>无模型的: 不知道 MDP 问题的转移和奖励函数 </li><li>可以从非完整的事件中学习, 借助自举法 </li><li>根据一个猜测值更新另一个猜测值</li></ol><p>TD学习算法：</p><ul><li>调整价值 $(S_t)$ 向估计的回报 $R_{t+1} + \gamma V(S_{t+1})$ 逼近 $V(S_t) \leftarrow V(S_t) + α(R_{t+1} + \gamma V(S_{t+1})−V(S_t))$ </li><li>$R_{t+1} + \gamma V(S_{t+1})$ 称为TD 目标 </li><li>$\delta_t = R_{t+1} + \gamma V(S_{t+1})−V(S_t)$ 称为TD 误差</li></ul><p>MC 与 TD 对比</p><ul><li>TD 可以在智能体运行过程中的<strong>每一步</strong>在线学习； MC 需要<strong>完整的事件序列</strong>计算出回报后学习 </li><li>TD 可以从<strong>不完整</strong>的事件序列中学习， MC 要求事件达到<strong>终止状态或序列足够长</strong></li><li>TD 是 低方差, 有偏差；MC 是 高方差, 零偏差</li><li>TD 能够利用马尔可夫性，因此在马尔可夫环境下更有效； MC 能利用马尔可夫性因此在非马尔可夫环境下更有效（只考虑reward）</li></ul><p>P90 AB问题：假如应用MC算法，由于需要完整的episode，因此，只有episode 1 能够用来计算A的状态值，所以显然，V(A) = 0；同时B状态的价值为6/8。而对于TD算法来说，由于状态A的后继有状态B（A-&gt;B是100%；B-&gt;1是75%），所以状态A的价值是通过状态B的价值来计算的。所以根据上面TD的计算公式，V(A)=V(B) = 6/8</p><p>自举法 : 更新时包含一个猜测量；采样法 : 使用采样的数据计算期望</p><ul><li>MC 不使用自举，使用采样</li><li>DP 使用自举，不使用采样</li><li>TD 使用自举，使用采样</li></ul><p>$\lambda -回报$： 每项的权重$(1-\lambda)\lambda^{n-1}$<strong>前向更新</strong><br>$$<br>G_t^{\lambda}=(1-\lambda)\sum_{n=1}^{\infty}\lambda^{n-1}G_t^{(n)}\<br>V(S_t)\leftarrow V(S_t)+\alpha(G_t^{\lambda}-V(S_t))<br>$$</p><p>资格迹：受频率启发；受近因启发（二者结合）更新量与 TD 误差 和资格迹 $E_t(s)$ 呈正比 <strong>后向更新</strong><br>$$<br>\delta_t = R_{t+1}+\gamma V(S_{t+1})-V(S_t)\<br>V(s)\leftarrow V(s) + \alpha\delta_tE_t(s)<br>$$</p><p>TD(1) 大致等价于每次经过的 MC 方法,误差会随在线运行逐步累积,如果价值函数只在事件结束时离线更新, 那么所有的更新量等同于 MC 方法</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;动态规划&quot;&gt;&lt;a href=&quot;#动态规划&quot; class=&quot;headerlink&quot; title=&quot;动态规划&quot;&gt;&lt;/a&gt;动态规划&lt;/h3&gt;&lt;p&gt;通过把原问题分解为相对简单的子问题来求解复杂问题 &lt;/p&gt;
&lt;p&gt;马尔可夫决策过程满足如下两个属性 &lt;/p&gt;
&lt;ul&gt;
&lt;l
      
    
    </summary>
    
      <category term="RL" scheme="http://www.liufr.com/categories/RL/"/>
    
    
      <category term="RL" scheme="http://www.liufr.com/tags/RL/"/>
    
  </entry>
  
  <entry>
    <title>强化学习(1)</title>
    <link href="http://www.liufr.com/2019/05/27/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/2019-05-27-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0(1)/"/>
    <id>http://www.liufr.com/2019/05/27/强化学习/2019-05-27-强化学习(1)/</id>
    <published>2019-05-26T16:00:00.000Z</published>
    <updated>2019-05-29T07:30:49.759Z</updated>
    
    <content type="html"><![CDATA[<p>强化学习考虑的是<strong>序贯决策过程</strong>(<strong>智能体</strong>处在特定的<strong>环境</strong>中产生一系列的<strong>动作</strong>，而环境能够根据这些动作改变智能体的当前<strong>状态</strong>。) 根据环境反馈的<strong>奖励</strong>，调整智能体的行为策略，提升智能体 实现<strong>目标</strong>的能力。</p><p>强化学习是基于奖励假设：所有的目标都可以通过最大化期望累加奖励实现</p><table><thead><tr><th align="right">强化学习</th><th align="right">监督/非监督学习</th></tr></thead><tbody><tr><td align="right">产生的结果能够改变数据的分布</td><td align="right">产生的结果 (输出) 不会改变数据的分布</td></tr><tr><td align="right">最终的目标可能要很长时间才能观察到</td><td align="right">结果是瞬时的</td></tr><tr><td align="right">没有明确的标签数据</td><td align="right">要么有明确的标签数据 (SL)</td></tr><tr><td align="right">根据当前的奖励，实现长远的目标</td><td align="right">要么完全没有任何标签数据 (USL)</td></tr></tbody></table><hr><h3 id="马尔科夫性-RL主要研究的问题"><a href="#马尔科夫性-RL主要研究的问题" class="headerlink" title="马尔科夫性(RL主要研究的问题)"></a>马尔科夫性(RL主要研究的问题)</h3><p>智能体未来的状态只与当前时刻的状态$S_t$有关, 而与过去的状态${S_1,…,S_{t−1}}$ 无关，那么称智能体的模型具有马尔可夫性。 </p><p>对于一个马尔可夫状态 $s$ 和后继状态 $s′$，状态转移概率定义为$P_{ss’} = \mathbb{P}[S_{t+1}=s’|S_t=s]$,状态转移矩阵定义为<br>$$<br>P = from\begin{bmatrix}<br>P_{11}&amp;…&amp;P_{1n} \<br>… &amp; \<br>P_{n1} &amp;… &amp; P_{nn}<br>\end{bmatrix}<br>$$<br>其中矩阵的每一行和均为1。</p><p><img src="pictures/markov.jpg" alt="markov"></p><p><strong>马尔科夫奖励过程</strong>=马尔科夫链+奖励（$&lt;S,P,R,\gamma&gt;$,$S$有限状态集$P$状态转移概率矩阵$R$奖励函数$\gamma$折扣因子）</p><p>回报$G_t = R_{t+1}+\gamma R_{t+2}+… = \sum_{k=0}^\infty \gamma^kR_{t+k+1}$，这种定义形式更重视近期的奖励，忽视远期的奖励，且$\lambda$越大回报越长远。</p><p><strong>状态价值函数</strong> 等于从状态s出发的期望回报 $v(s)=\mathbb{E}[G_t|S_t=s]$，分为瞬间奖励$R_{t+1}$以及后续状态的折扣价值$\lambda v(S_{t+1})$<br>$$<br>v(s) = \mathbb{E}[G_t|S_t=s]<br>=\mathbb{E}[R_{t+1}+\lambda v(S_{t+1})|S_t=s]\<br>v(s) = R_s + \gamma \sum_{s’\in S}P_{ss’}v(s’)<br>$$<br>矩阵形式的贝尔曼方程<br>$$<br>v = R + \lambda Pv; v=(I-\lambda P)^{-1}R<br>$$</p><p><strong>马尔可夫决策过程$&lt;S,A,P,R,\lambda&gt;$(A有限动作集)</strong></p><p>一个强化学习的智能体可能包括如下一个或多个元素 </p><ul><li>确定性/随机性策略：智能体的行为(状态到动作的映射)，与历史无关，静态性$A_t\sim\pi(\cdot|S_t),\forall t&gt;0$</li><li>价值函数（值函数、性能指标函数）：智能体在某一状态和/或某一动作时是好还是坏</li><li>模型：智能体对真实环境的估计，预测下一时刻的状态$P_{ss’}^a$和奖励$R_s^a$</li></ul><p><strong>策略</strong> ： 状态到动作的一种分布$\pi (a|s)=\mathbb{P}[A_t=a|S_t=s]$,马尔科夫奖励过程$&lt;S,P^{\pi},R^{\pi},\lambda&gt;,P_{ss’}^{\pi} = \sum_{a\in A}\pi (a|s)P_{ss’}^a,R_s^{\pi}=\sum_{a\in A}\pi(a|s)R_s^a$</p><p><strong>状态-价值函数</strong> 从状态s出发, 在策略 $\pi$ 作用下的期望回报$v_{\pi}(s)=\mathbb{E}_\pi[G_t|S_t=s]$</p><p><strong>动作-价值函数</strong> 从状态s出发, 首先执行动作 a, 然后在策略$\pi$作用下的期望回报$q_\pi(s,a)=\mathbb E_{\pi}[G_t|S_t=s,A_t=a]$</p><hr><h3 id="最优价值函数-智能体在MDP问题下最好的性能，表示该问题可解"><a href="#最优价值函数-智能体在MDP问题下最好的性能，表示该问题可解" class="headerlink" title="最优价值函数(智能体在MDP问题下最好的性能，表示该问题可解)"></a>最优价值函数(智能体在MDP问题下最好的性能，表示该问题可解)</h3><p><strong>最优状态-价值函数</strong>在所有策略中价值最大的$v_*(s)=max_{\pi}v_{\pi}(s)$</p><p><strong>最优动作-价值函数</strong>在所有策略中动作价值函数最大的$q_*(s,a)=max_{\pi}q_{\pi}(s,a)$</p><p>总是存在一个最优策略，通过最大化$q_<em>(s,a)$来确定<br>$$<br>\pi_</em>(a|s)\left{\begin{matrix}<br>1 &amp; if\ a=argmax_{a\in A}q_*(s,a)\<br>0 &amp; otherwise<br>\end{matrix}\right.<br>$$</p><p>强化学习目标是找到一组时间序列的动作${A_0,A_1,…}$，使得智能体从$S_0$出发得到的期望累加奖励最大化$v*(s_0)=\mathbb E[max_{A_0,A_1,…}(R_1+\lambda R_2+\lambda^2R_3…)]$</p><p>最优策略性质：以第一步决策所形成的阶段和状态作为初始条件来 考虑时，余下的决策对余下的问题而言也必构成最优策略 </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;强化学习考虑的是&lt;strong&gt;序贯决策过程&lt;/strong&gt;(&lt;strong&gt;智能体&lt;/strong&gt;处在特定的&lt;strong&gt;环境&lt;/strong&gt;中产生一系列的&lt;strong&gt;动作&lt;/strong&gt;，而环境能够根据这些动作改变智能体的当前&lt;strong&gt;状态&lt;/str
      
    
    </summary>
    
      <category term="RL" scheme="http://www.liufr.com/categories/RL/"/>
    
    
      <category term="RL" scheme="http://www.liufr.com/tags/RL/"/>
    
  </entry>
  
  <entry>
    <title>互联网搜索与排序</title>
    <link href="http://www.liufr.com/2018/12/05/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/2018-12-05-%E4%BA%92%E8%81%94%E7%BD%91%E6%90%9C%E7%B4%A2%E4%B8%8E%E6%8E%92%E5%BA%8F/"/>
    <id>http://www.liufr.com/2018/12/05/数据挖掘/2018-12-05-互联网搜索与排序/</id>
    <published>2018-12-04T16:00:00.000Z</published>
    <updated>2018-12-09T09:12:43.520Z</updated>
    
    <content type="html"><![CDATA[<h2 id="向量空间模型（VSM）"><a href="#向量空间模型（VSM）" class="headerlink" title="向量空间模型（VSM）"></a>向量空间模型（VSM）</h2><ul><li>将查询字符串表达为带权重的tf-idf向量(查询向量)</li><li>类似,将文档字符串表达为带权重tf-idf向量(文档向量)</li><li>计算查询查询向量和文档向量的余弦相似度</li><li>将文档按照其与查询的相似度分值从大到小进行排序</li><li>返回前K个(e.g.,K=10)文档并展示给用户</li></ul><h3 id="tf-idf"><a href="#tf-idf" class="headerlink" title="tf-idf"></a>tf-idf</h3><p>衡量某一个词在文档中的重要性</p><ul><li>tf(w,d):term frequency 词w在d中出现的次数，tf值越大，w在文档d中越重要。</li><li>df(w): document frequency 整个数据集中，包含w文档的个数，df越大，w越不重要。</li><li>idf(w): inverse document frequency $idf = log \frac{N}{df}$ 其中N为文档集的个数</li><li>tf-idf(w,d) = tf(w,d)*idf(w)</li></ul><h3 id="BM25-Best-match-25"><a href="#BM25-Best-match-25" class="headerlink" title="BM25(Best match 25)"></a>BM25(Best match 25)</h3><p>$$<br>BM25 = \sum_{i \in q} \log\frac{N}{df_i} \cdot \frac{(k_1+1)tf_i}{k_1((1-b)+b\frac{dl}{avdl})+tf_i}<br>$$</p><ul><li>avgdl:集合中平均文档长度</li><li>$k_1$:控制因$tf$的增大最终排序值的速度<ul><li>k1=0:二值模型,只反映词是否出现,不考虑出现次数</li><li>k1无穷大:反映真正的tf值</li></ul></li><li>b:控制文档长度归一化程度<ul><li>b=0:不考虑文档长度对最终分值的影响</li><li>b=1:考虑文档长度平均文档长度的相对值</li></ul></li><li>经验值:k1=1.2~2,b=0.75</li></ul><p>在上述公式中，文档长度定义为$dl = \sum_{i\in V}tf_i$，文档长度归一化部分为$(1-b)+b\frac{dl}{avdl}, 0\leq b \leq 1$ 当$b=0$时，不进行归一化，当$b=1$时，全文档进行归一化。</p><hr><h2 id="排序评价指标"><a href="#排序评价指标" class="headerlink" title="排序评价指标"></a>排序评价指标</h2><h3 id="P-K-Precision-at-K"><a href="#P-K-Precision-at-K" class="headerlink" title="P@K(Precision at K)"></a>P@K(Precision at K)</h3><ul><li>设置一个排序位置K</li><li>前K个位置相关文档所占的比例</li><li>忽略K位置之后的所有值</li></ul><h3 id="MAP-Mean-average-precision"><a href="#MAP-Mean-average-precision" class="headerlink" title="MAP(Mean average precision)"></a>MAP(Mean average precision)</h3><ul><li>只考虑出现过相关文档的位置</li></ul><p><img src="https://ws4.sinaimg.cn/large/006tNbRwly1fxw5h5m1v9j311o0dyt9n.jpg" alt></p><h3 id="DCG-Discounted-Cumulative-Gain"><a href="#DCG-Discounted-Cumulative-Gain" class="headerlink" title="DCG(Discounted Cumulative Gain)"></a>DCG(Discounted Cumulative Gain)</h3><ul><li><strong>Gain</strong>: 文档对用户Gain与其查询相关度有关，$2^{label}-1$，其中$label$取值如bad(0),fair(1),good(3),excellent(7),perfect(15)等，也就是说Gain是一个指数函数，效果越好影响越大。</li><li><strong>Discounted Cumulative Gain</strong>：由于返回结果有一个排序，因此根据所在排序还需要一定的打折$\frac{1}{log_2(rank+1)}$</li></ul><p>比较二者那么<br><strong>CG@N</strong> $CG=r_1+r_2+…+r_n$<br><strong>NCG@N</strong> $NCG= \frac{r_1}{\log(1+1)} + \frac{r_2}{\log(1+2)}+…+\frac{r_n}{\log(1+n)}$</p><h3 id="NDCG-Normalized-DCG"><a href="#NDCG-Normalized-DCG" class="headerlink" title="NDCG(Normalized DCG)"></a>NDCG(Normalized DCG)</h3><p>利用最优排序对DCG@N进行归一化处理，其中最优排序为按照用户标注对文档进行排序。其中$IDCG_p$为理想情况下最大的$DCG$值。<br>$$<br>nDCG_p = \frac{DCG_p}{IDCG_p}<br>$$</p><h2 id="Pair-wise排序学习：区分文档间的差异"><a href="#Pair-wise排序学习：区分文档间的差异" class="headerlink" title="Pair-wise排序学习：区分文档间的差异"></a>Pair-wise排序学习：区分文档间的差异</h2><p>核心思想：模型只需要区分同一个查询内部标注为不同相关度文档间的差异</p><h3 id="排序支持向量机（ranking-SVM）"><a href="#排序支持向量机（ranking-SVM）" class="headerlink" title="排序支持向量机（ranking SVM）"></a>排序支持向量机（ranking SVM）</h3><ul><li>Ranking SVM 训练<ul><li>构造训练数据集合${\phi(q_k,d_{ki})-\phi(q_k,d_{kj}),+1}$</li><li>训练二值分类SVM，得到打分函数$f(q,d)=&lt;w,\phi(q,d)&gt;$</li></ul></li><li>Ranking SVM 在线应用<ul><li>给定一个查询q和检索出的文档集合$C={d_i}$</li><li>用$f(q,d)$对每一个C中的文档进行打分</li><li>将C中的文档按照$f(q,d)$从大到小排序</li></ul></li></ul><p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fxwtpbeiuhj31030u0jut.jpg" alt></p><p>优点</p><ul><li>只关注文档间的差异性，解决了查询的差异化问题</li><li>实际应用效果良好</li></ul><p>缺点</p><ul><li>N个文档有$O(N^2)$文档有序对，复杂度高</li><li>没有考虑浏览特性</li><li>违背分类训练数据独立同分布假设</li></ul><hr><h2 id="语言排序模型"><a href="#语言排序模型" class="headerlink" title="语言排序模型"></a>语言排序模型</h2><p>词袋假设$P(w_1,w_2,…w_m)=P(w_1)P(w_2)…P(w_m)$<br>估计每一个词的出现概率$P(w)=\frac{#w}{all,words}$，其中$\sum_{w\in V}P(w)=1$ </p><p>语言模型中的马尔科夫假设，$P(w_n|w_{n-1},w_{n-2},…,w_1)=P(w_n|w_{n-1})$<br>$P(w_1,w_2,…,w_m)=P(w_1)P(w_2|w_1)P(w_3|w_2)…P(w_m|w_{m-1})$<br>给定一个训练文档集合，统计给个一个词之后，其他词出现的概率$P(w_i|w_j)=\frac{#(w_jw_i)}{#w_j}$<br>举例来说$\sum_{w_i\in V}P(w_i|’is’)=1$</p><h3 id="数据稀疏问题"><a href="#数据稀疏问题" class="headerlink" title="数据稀疏问题"></a>数据稀疏问题</h3><p>对于0概率问题采用平滑化方法</p><h3 id="LM-for-IR"><a href="#LM-for-IR" class="headerlink" title="LM for IR"></a>LM for IR</h3><ul><li>定义生成模型的细节</li><li>估计模型参数$P(w|d)$</li><li>平滑化，防止零概率</li><li>将文档对应的生成模型应用于查询，计算生成概率</li><li>按照生成概率将文档排序，取topN展现给用户</li></ul><p>给定查询$q$和文档$d$，对文档的打分为$P(d|q)$根据贝叶斯公式<br>$$<br>P(d|q) = \frac{P(q|d)P(d)}{P(q)}<br>$$<br>对于独立假设来说$P(q|d)=P(q_1q_2,….q_m|d)=\Pi_{i=1}^MP(q_i|d)$</p><p>举例：<br>$q$ 中国_科学院_大学<br>$d$ 科学院_大学_计算机_学院</p><p>$P(q|d)=P(中国|d)P(科学院|d)P(大学|d)=0<em>0.25</em>0.25=0$</p><h3 id="Dirichlet-平滑-与混合平滑"><a href="#Dirichlet-平滑-与混合平滑" class="headerlink" title="Dirichlet 平滑 与混合平滑"></a>Dirichlet 平滑 与混合平滑</h3><p>D={d1,d2}, Query q: Michael Jackson</p><p>d1: Jackson was one of the most talented entertainers of all time（11）<br>d2: Michael Jackson anointed himself King of Pop（7）<br>$P(q|d_1)=\frac{0}{11}<em>\frac{1}{11}=0,P(q|d_2)=\frac{1}{7}</em>\frac{1}{7}=\frac{1}{49},P(Michael|C)=\frac{1}{18},P(Jackson|C)=\frac{2}{18}$</p><p><strong>混合模型</strong>$\lambda=0.5$<br>$P_{mix}(q|d_1)=(\frac{0}{11}<em>\frac{1}{2}+\frac{1}{18}*\frac{1}{2})</em>(\frac{1}{11}<em>\frac{1}{2}+\frac{2}{18}<em>\frac{1}{2})$<br>$P_{mix}(q|d_2)=(\frac{1}{7}</em>\frac{1}{2}+\frac{1}{18}*\frac{1}{2})</em>(\frac{1}{7}<em>\frac{1}{2}+\frac{2}{18}</em>\frac{1}{2})$<br><strong>狄里克莱平滑</strong>$\mu = 5$<br>$P_{dir}(q|d_1)=(\frac{0+\frac{5}{18}}{11+5})(\frac{1+5<em>\frac{2}{18}}{11+5})$<br>$P_{dir}(q|d_2)=(\frac{1+\frac{5}{18}}{7+5})(\frac{1+5</em>\frac{2}{18}}{7+5})$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;向量空间模型（VSM）&quot;&gt;&lt;a href=&quot;#向量空间模型（VSM）&quot; class=&quot;headerlink&quot; title=&quot;向量空间模型（VSM）&quot;&gt;&lt;/a&gt;向量空间模型（VSM）&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;将查询字符串表达为带权重的tf-idf向量(查询向量)&lt;
      
    
    </summary>
    
      <category term="Data mining" scheme="http://www.liufr.com/categories/Data-mining/"/>
    
    
      <category term="Data mining" scheme="http://www.liufr.com/tags/Data-mining/"/>
    
  </entry>
  
</feed>
